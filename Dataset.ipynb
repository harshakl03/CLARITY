{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c647c7-381e-4d13-90cb-3e4be807038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset at: D:\\Projects\\CLARITY\\Model\\Dataset\\archive\n",
      "==================================================\n",
      "📁 Folders found (12):\n",
      "   └── images_001\n",
      "   └── images_002\n",
      "   └── images_003\n",
      "   └── images_004\n",
      "   └── images_005\n",
      "   └── images_006\n",
      "   └── images_007\n",
      "   └── images_008\n",
      "   └── images_009\n",
      "   └── images_010\n",
      "   └── images_011\n",
      "   └── images_012\n",
      "\n",
      "📄 Files found (8):\n",
      "   └── ARXIV_V5_CHESTXRAY.pdf\n",
      "   └── BBox_List_2017.csv\n",
      "   └── Data_Entry_2017.csv\n",
      "   └── FAQ_CHESTXRAY.pdf\n",
      "   └── LOG_CHESTXRAY.pdf\n",
      "   └── README_CHESTXRAY.pdf\n",
      "   └── test_list.txt\n",
      "   └── train_val_list.txt\n",
      "\n",
      "✅ Metadata file found: ['Data_Entry_2017.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def check_dataset_structure(dataset_path):\n",
    "    \"\"\"Check the structure of your local NIH dataset\"\"\"\n",
    "    \n",
    "    dataset_path = Path(dataset_path)\n",
    "    print(f\"Checking dataset at: {dataset_path.absolute()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if path exists\n",
    "    if not dataset_path.exists():\n",
    "        print(\"❌ Dataset path does not exist!\")\n",
    "        return False\n",
    "    \n",
    "    # Look for common files and folders\n",
    "    files_found = []\n",
    "    folders_found = []\n",
    "    \n",
    "    for item in dataset_path.iterdir():\n",
    "        if item.is_file():\n",
    "            files_found.append(item.name)\n",
    "        else:\n",
    "            folders_found.append(item.name)\n",
    "    \n",
    "    print(f\"📁 Folders found ({len(folders_found)}):\")\n",
    "    for folder in sorted(folders_found):\n",
    "        print(f\"   └── {folder}\")\n",
    "    \n",
    "    print(f\"\\n📄 Files found ({len(files_found)}):\")\n",
    "    for file in sorted(files_found):\n",
    "        print(f\"   └── {file}\")\n",
    "    \n",
    "    # Check for metadata file\n",
    "    metadata_files = [f for f in files_found if 'Data_Entry' in f or 'metadata' in f.lower()]\n",
    "    if metadata_files:\n",
    "        print(f\"\\n✅ Metadata file found: {metadata_files}\")\n",
    "        return metadata_files\n",
    "    else:\n",
    "        print(\"\\n⚠️  No metadata file found. Looking for CSV files...\")\n",
    "        csv_files = [f for f in files_found if f.endswith('.csv')]\n",
    "        if csv_files:\n",
    "            print(f\"📊 CSV files: {csv_files}\")\n",
    "            return csv_files\n",
    "        else:\n",
    "            print(\"❌ No CSV metadata file found!\")\n",
    "            return None\n",
    "\n",
    "# Usage - Update this path to your dataset location\n",
    "dataset_path = r\"D:\\Projects\\CLARITY\\Model\\Dataset\\archive\"  # Update this path!\n",
    "metadata_file = check_dataset_structure(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a4d2db5-e988-46a3-85d5-dd71a07c3419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Exploring dataset at: D:\\Projects\\CLARITY\\Model\\Dataset\\archive\n",
      "============================================================\n",
      "📊 Metadata file found: BBox_List_2017.csv\n",
      "\n",
      "📁 Folder structure analysis:\n",
      "----------------------------------------\n",
      "\n",
      "📂 images_001/\n",
      "   └── images/\n",
      "       └── 4,999 images\n",
      "\n",
      "📂 images_002/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_003/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_004/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_005/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_006/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_007/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_008/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_009/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_010/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_011/\n",
      "   └── images/\n",
      "       └── 10,000 images\n",
      "\n",
      "📂 images_012/\n",
      "   └── images/\n",
      "       └── 7,121 images\n",
      "----------------------------------------\n",
      "🖼️  TOTAL IMAGES: 112,120\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def explore_nested_dataset_structure(base_path):\n",
    "    \"\"\"Explore your nested dataset structure\"\"\"\n",
    "    \n",
    "    base_path = Path(base_path)\n",
    "    print(f\"🔍 Exploring dataset at: {base_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(\"❌ Path does not exist!\")\n",
    "        return None\n",
    "    \n",
    "    # Check for CSV file\n",
    "    csv_files = list(base_path.glob(\"*.csv\"))\n",
    "    metadata_file = None\n",
    "    \n",
    "    if csv_files:\n",
    "        metadata_file = csv_files[0]  # Select the first CSV file found\n",
    "        print(f\"📊 Metadata file found: {metadata_file.name}\")\n",
    "    else:\n",
    "        print(\"❌ No CSV metadata file found!\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    # Explore nested structure\n",
    "    total_images = 0\n",
    "    folder_structure = {}\n",
    "    \n",
    "    print(f\"\\n📁 Folder structure analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for main_folder in sorted(base_path.iterdir()):\n",
    "        if main_folder.is_dir() and main_folder.name.startswith('images_'):\n",
    "            print(f\"\\n📂 {main_folder.name}/\")\n",
    "            \n",
    "            # Look for subfolders\n",
    "            subfolders = [f for f in main_folder.iterdir() if f.is_dir()]\n",
    "            \n",
    "            if subfolders:\n",
    "                for subfolder in subfolders:\n",
    "                    print(f\"   └── {subfolder.name}/\")\n",
    "                    \n",
    "                    # Count images in this subfolder\n",
    "                    image_extensions = ['*.png', '*.jpg', '*.jpeg']\n",
    "                    subfolder_images = 0\n",
    "                    \n",
    "                    for ext in image_extensions:\n",
    "                        subfolder_images += len(list(subfolder.glob(ext)))\n",
    "                    \n",
    "                    print(f\"       └── {subfolder_images:,} images\")\n",
    "                    \n",
    "                    folder_structure[f\"{main_folder.name}/{subfolder.name}\"] = {\n",
    "                        'path': subfolder,\n",
    "                        'count': subfolder_images\n",
    "                    }\n",
    "                    \n",
    "                    total_images += subfolder_images\n",
    "            else:\n",
    "                print(\"   └── (no subfolders)\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"🖼️  TOTAL IMAGES: {total_images:,}\")\n",
    "    \n",
    "    return metadata_file, folder_structure, total_images\n",
    "\n",
    "# Run exploration\n",
    "BASE_PATH = r\"D:\\Projects\\CLARITY\\Model\\Dataset\\archive\"\n",
    "metadata_file, folder_structure, total_images = explore_nested_dataset_structure(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67795333-547f-4005-833c-a6bfff27cacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Expected unique images from metadata: 112,120\n",
      "🔍 Scanning images_001...\n",
      "🔍 Scanning images_002...\n",
      "🔍 Scanning images_003...\n",
      "🔍 Scanning images_004...\n",
      "🔍 Scanning images_005...\n",
      "🔍 Scanning images_006...\n",
      "🔍 Scanning images_007...\n",
      "🔍 Scanning images_008...\n",
      "🔍 Scanning images_009...\n",
      "🔍 Scanning images_010...\n",
      "🔍 Scanning images_011...\n",
      "🔍 Scanning images_012...\n",
      "📊 Actual unique image names found: 0\n",
      "📊 Total image files found: 0\n",
      "\n",
      "🔄 Duplicate Analysis:\n",
      "   Images with duplicates: 0\n",
      "   Images with single copy: 0\n",
      "\n",
      "📋 Metadata Alignment:\n",
      "   Images in metadata but missing from disk: 112,120\n",
      "   Images on disk but not in metadata: 0\n",
      "   First 5 missing: ['00007352_002.png', '00010426_000.png', '00019313_012.png', '00014303_000.png', '00016134_013.png']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def analyze_dataset_duplicates(dataset_path):\n",
    "    \"\"\"Analyze the duplicate situation in your dataset\"\"\"\n",
    "    \n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Load metadata to get expected image names\n",
    "    metadata_path = dataset_path / \"Data_Entry_2017.csv\"\n",
    "    if not metadata_path.exists():\n",
    "        print(\"❌ Data_Entry_2017.csv not found!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(metadata_path)\n",
    "    expected_images = set(df['Image Index'].tolist())\n",
    "    \n",
    "    print(f\"📊 Expected unique images from metadata: {len(expected_images):,}\")\n",
    "    \n",
    "    # Find all actual images on disk\n",
    "    found_images = defaultdict(list)  # image_name -> [list of paths]\n",
    "    \n",
    "    for folder in dataset_path.iterdir():\n",
    "        if folder.is_dir() and folder.name.startswith('images_'):\n",
    "            print(f\"🔍 Scanning {folder.name}...\")\n",
    "            \n",
    "            for img_file in folder.glob(\"*.png\"):\n",
    "                image_name = img_file.name\n",
    "                found_images[image_name].append(img_file)\n",
    "    \n",
    "    print(f\"📊 Actual unique image names found: {len(found_images):,}\")\n",
    "    print(f\"📊 Total image files found: {sum(len(paths) for paths in found_images.values()):,}\")\n",
    "    \n",
    "    # Analyze duplication patterns\n",
    "    duplicates = {name: paths for name, paths in found_images.items() if len(paths) > 1}\n",
    "    \n",
    "    print(f\"\\n🔄 Duplicate Analysis:\")\n",
    "    print(f\"   Images with duplicates: {len(duplicates):,}\")\n",
    "    print(f\"   Images with single copy: {len(found_images) - len(duplicates):,}\")\n",
    "    \n",
    "    if len(duplicates) > 0:\n",
    "        duplicate_counts = defaultdict(int)\n",
    "        for paths in duplicates.values():\n",
    "            duplicate_counts[len(paths)] += 1\n",
    "        \n",
    "        print(f\"\\n   Duplication pattern:\")\n",
    "        for count, num_images in duplicate_counts.items():\n",
    "            print(f\"     {count} copies: {num_images:,} images\")\n",
    "    \n",
    "    # Check for missing images\n",
    "    missing_images = expected_images - set(found_images.keys())\n",
    "    extra_images = set(found_images.keys()) - expected_images\n",
    "    \n",
    "    print(f\"\\n📋 Metadata Alignment:\")\n",
    "    print(f\"   Images in metadata but missing from disk: {len(missing_images):,}\")\n",
    "    print(f\"   Images on disk but not in metadata: {len(extra_images):,}\")\n",
    "    \n",
    "    if len(missing_images) > 0:\n",
    "        print(f\"   First 5 missing: {list(missing_images)[:5]}\")\n",
    "    \n",
    "    if len(extra_images) > 0:\n",
    "        print(f\"   First 5 extra: {list(extra_images)[:5]}\")\n",
    "    \n",
    "    return found_images, duplicates, missing_images\n",
    "\n",
    "# Run the analysis\n",
    "dataset_path = r\"D:\\Projects\\CLARITY\\Model\\Dataset\\archive\"  # Update your path!\n",
    "found_images, duplicates, missing = analyze_dataset_duplicates(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45faa3b4-9465-4917-83e8-2edd890fae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗺️  Creating image mapping...\n",
      "📂 Processing images_001/images...\n",
      "📂 Processing images_002/images...\n",
      "📂 Processing images_003/images...\n",
      "📂 Processing images_004/images...\n",
      "📂 Processing images_005/images...\n",
      "📂 Processing images_006/images...\n",
      "📂 Processing images_007/images...\n",
      "📂 Processing images_008/images...\n",
      "📂 Processing images_009/images...\n",
      "📂 Processing images_010/images...\n",
      "📂 Processing images_011/images...\n",
      "📂 Processing images_012/images...\n",
      "✅ Image mapping complete: 112,120 unique images\n"
     ]
    }
   ],
   "source": [
    "def create_nested_image_mapping(base_path, folder_structure):\n",
    "    \"\"\"Create mapping from image names to paths in nested structure\"\"\"\n",
    "    \n",
    "    print(f\"\\n🗺️  Creating image mapping...\")\n",
    "    \n",
    "    image_mapping = {}\n",
    "    \n",
    "    for folder_key, folder_info in folder_structure.items():\n",
    "        folder_path = folder_info['path']\n",
    "        \n",
    "        print(f\"📂 Processing {folder_key}...\")\n",
    "        \n",
    "        # Get all images from this nested folder\n",
    "        for img_file in folder_path.glob(\"*.png\"):\n",
    "            image_name = img_file.name\n",
    "            \n",
    "            if image_name in image_mapping:\n",
    "                # Duplicate found - this explains your double count!\n",
    "                print(f\"⚠️  Duplicate found: {image_name}\")\n",
    "                print(f\"    Existing: {image_mapping[image_name]}\")\n",
    "                print(f\"    New: {img_file}\")\n",
    "            else:\n",
    "                image_mapping[image_name] = img_file\n",
    "    \n",
    "    print(f\"✅ Image mapping complete: {len(image_mapping):,} unique images\")\n",
    "    return image_mapping\n",
    "\n",
    "# Create the mapping\n",
    "if folder_structure:\n",
    "    image_mapping = create_nested_image_mapping(BASE_PATH, folder_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4c6b7-cd53-4738-a9ca-242a0395543b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
