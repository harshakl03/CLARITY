{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8b342da-2aa4-45e8-b661-bf2621f8f5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full Enhanced + Speed Optimized setup\n",
      "🚀 Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 3060\n",
      "   VRAM: 12.9 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Enhanced Setup and Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import enhanced modules\n",
    "from src.data.dataset import ChestXrayDataset, create_data_splits, calculate_class_weights\n",
    "from src.data.dataloader import create_rtx3060_dataloaders\n",
    "from src.models.densenet121_enhanced import create_enhanced_model_for_rtx3060\n",
    "from src.models.trainer_enhanced import EnhancedRTX3060Trainer\n",
    "\n",
    "print(\"✅ Full Enhanced + Speed Optimized setup\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e9d62c3-1988-4d23-b627-c7630bbe5576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading data pipeline...\n",
      "✅ Metadata loaded: 112,120 entries\n",
      "✅ Image mapping: 112,120 images\n",
      "Patient-level data splits:\n",
      "  Train: 83,847 images from 23,105 patients (74.8%)\n",
      "  Val:   11,550 images from 3,080 patients (10.3%)\n",
      "  Test:  16,723 images from 4,620 patients (14.9%)\n",
      "✅ No patient overlap verified - clean splits!\n",
      "✅ Data splits: Train(83,847) Val(11,550) Test(16,723)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data Pipeline\n",
    "BASE_PATH = Path(r\"D:/Projects/CLARITY/Model/Dataset/archive\")  # Update this!\n",
    "\n",
    "print(\"🔄 Loading data pipeline...\")\n",
    "\n",
    "# Load metadata\n",
    "data_entry_path = BASE_PATH / \"Data_Entry_2017.csv\"\n",
    "df = pd.read_csv(data_entry_path)\n",
    "print(f\"✅ Metadata loaded: {len(df):,} entries\")\n",
    "\n",
    "# Recreate image mapping\n",
    "image_mapping = {}\n",
    "for main_folder in sorted(BASE_PATH.iterdir()):\n",
    "    if main_folder.is_dir() and main_folder.name.startswith('images_'):\n",
    "        images_subfolder = main_folder / 'images'\n",
    "        if images_subfolder.exists():\n",
    "            for img_file in images_subfolder.glob(\"*.png\"):\n",
    "                image_name = img_file.name\n",
    "                if image_name not in image_mapping:\n",
    "                    image_mapping[image_name] = img_file\n",
    "\n",
    "print(f\"✅ Image mapping: {len(image_mapping):,} images\")\n",
    "\n",
    "# Create data splits (consistent with previous experiments)\n",
    "train_df, val_df, test_df = create_data_splits(df, \n",
    "                                               test_size=0.15,\n",
    "                                               val_size=0.10,\n",
    "                                               random_seed=42)\n",
    "\n",
    "print(f\"✅ Data splits: Train({len(train_df):,}) Val({len(val_df):,}) Test({len(test_df):,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2c3c90a-07e6-4008-8e46-3fbd64ca613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating full enhanced with speed optimizations...\n",
      "Dataset created with 83847 samples\n",
      "Training mode: True\n",
      "Image size: 448x448\n",
      "\n",
      "Label matrix created: (83847, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............  45146\n",
      "  Atelectasis..............   8720\n",
      "  Cardiomegaly.............   2019\n",
      "  Effusion.................  10071\n",
      "  Infiltration.............  14772\n",
      "  Mass.....................   4477\n",
      "  Nodule...................   4691\n",
      "  Pneumonia................   1062\n",
      "  Pneumothorax.............   3981\n",
      "  Consolidation............   3458\n",
      "  Edema....................   1738\n",
      "  Emphysema................   1794\n",
      "  Fibrosis.................   1236\n",
      "  Pleural_Thickening.......   2562\n",
      "  Hernia...................    171\n",
      "Transforms created for training\n",
      "Dataset created with 11550 samples\n",
      "Training mode: False\n",
      "Image size: 448x448\n",
      "\n",
      "Label matrix created: (11550, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............   6197\n",
      "  Atelectasis..............   1148\n",
      "  Cardiomegaly.............    331\n",
      "  Effusion.................   1311\n",
      "  Infiltration.............   2181\n",
      "  Mass.....................    470\n",
      "  Nodule...................    710\n",
      "  Pneumonia................    149\n",
      "  Pneumothorax.............    511\n",
      "  Consolidation............    517\n",
      "  Edema....................    232\n",
      "  Emphysema................    305\n",
      "  Fibrosis.................    169\n",
      "  Pleural_Thickening.......    326\n",
      "  Hernia...................     13\n",
      "Transforms created for validation\n",
      "Dataset created with 16723 samples\n",
      "Training mode: False\n",
      "Image size: 448x448\n",
      "\n",
      "Label matrix created: (16723, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............   9018\n",
      "  Atelectasis..............   1691\n",
      "  Cardiomegaly.............    426\n",
      "  Effusion.................   1935\n",
      "  Infiltration.............   2941\n",
      "  Mass.....................    835\n",
      "  Nodule...................    930\n",
      "  Pneumonia................    220\n",
      "  Pneumothorax.............    810\n",
      "  Consolidation............    692\n",
      "  Edema....................    333\n",
      "  Emphysema................    417\n",
      "  Fibrosis.................    281\n",
      "  Pleural_Thickening.......    497\n",
      "  Hernia...................     43\n",
      "Transforms created for validation\n",
      "✅ Full enhanced datasets created!\n",
      "   Resolution: 448×448\n",
      "   Augmentation: 85%\n",
      "   Expected: ~12-15 min/epoch (vs 30 min at 512px)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Full Enhanced + Speed Optimized Settings\n",
    "print(\"🔄 Creating full enhanced with speed optimizations...\")\n",
    "\n",
    "# OPTIMIZED SETTINGS FOR FULL ENHANCED\n",
    "OPTIMAL_IMAGE_SIZE = 448    # Sweet spot: better than 384, faster than 512\n",
    "OPTIMAL_BATCH_SIZE = 8      # Optimal for 448px on RTX 3060\n",
    "OPTIMAL_WORKERS = 4         # Balanced CPU usage\n",
    "OPTIMAL_ACCUMULATION = 3    # Effective batch = 24\n",
    "\n",
    "# Create datasets with optimized settings\n",
    "train_dataset = ChestXrayDataset(\n",
    "    df=train_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=OPTIMAL_IMAGE_SIZE,\n",
    "    is_training=True,\n",
    "    augmentation_prob=0.85  # Strong augmentation for better generalization\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDataset(\n",
    "    df=val_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=OPTIMAL_IMAGE_SIZE,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDataset(\n",
    "    df=test_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=OPTIMAL_IMAGE_SIZE,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(f\"✅ Full enhanced datasets created!\")\n",
    "print(f\"   Resolution: {OPTIMAL_IMAGE_SIZE}×{OPTIMAL_IMAGE_SIZE}\")\n",
    "print(f\"   Augmentation: 85%\")\n",
    "print(f\"   Expected: ~12-15 min/epoch (vs 30 min at 512px)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6805cb9-a582-4f25-8626-155bd409b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating optimized dataloaders...\n",
      "\n",
      "Class weights (inverse_freq):\n",
      "--------------------------------------------------\n",
      "No Finding...............    0.031 (pos:  45146)\n",
      "Atelectasis..............    0.158 (pos:   8720)\n",
      "Cardiomegaly.............    0.684 (pos:   2019)\n",
      "Effusion.................    0.137 (pos:  10071)\n",
      "Infiltration.............    0.093 (pos:  14772)\n",
      "Mass.....................    0.308 (pos:   4477)\n",
      "Nodule...................    0.294 (pos:   4691)\n",
      "Pneumonia................    1.299 (pos:   1062)\n",
      "Pneumothorax.............    0.347 (pos:   3981)\n",
      "Consolidation............    0.399 (pos:   3458)\n",
      "Edema....................    0.794 (pos:   1738)\n",
      "Emphysema................    0.769 (pos:   1794)\n",
      "Fibrosis.................    1.116 (pos:   1236)\n",
      "Pleural_Thickening.......    0.539 (pos:   2562)\n",
      "Hernia...................    8.030 (pos:    171)\n",
      "Creating RTX 3060 optimized dataloaders:\n",
      "  Batch size: 8\n",
      "  Num workers: 4\n",
      "  Weighted sampling: True\n",
      "  Weighted sampler created with 83847 samples\n",
      "DataLoaders created:\n",
      "  Train: 10480 batches\n",
      "  Val:   1444 batches\n",
      "  Test:  2091 batches\n",
      "\n",
      "📊 Optimized Memory Usage:\n",
      "   Per batch: ~0.02 GB\n",
      "   Effective batch: 24\n",
      "   Expected time: 12-15 min/epoch\n",
      "   Memory safe: ✅ (0.1GB < 12GB)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Speed-Optimized DataLoaders for Full Enhanced\n",
    "print(\"🔄 Creating optimized dataloaders...\")\n",
    "\n",
    "# Enhanced class weights\n",
    "class_weights = calculate_class_weights(train_dataset.labels, method='inverse_freq')\n",
    "\n",
    "# Create optimized dataloaders\n",
    "train_loader, val_loader, test_loader = create_rtx3060_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=OPTIMAL_BATCH_SIZE,\n",
    "    num_workers=OPTIMAL_WORKERS,\n",
    "    use_weighted_sampling=True\n",
    ")\n",
    "\n",
    "# Memory calculation\n",
    "memory_per_batch_gb = (OPTIMAL_BATCH_SIZE * 3 * OPTIMAL_IMAGE_SIZE * OPTIMAL_IMAGE_SIZE * 4) / 1e9\n",
    "print(f\"\\n📊 Optimized Memory Usage:\")\n",
    "print(f\"   Per batch: ~{memory_per_batch_gb:.2f} GB\")\n",
    "print(f\"   Effective batch: {OPTIMAL_BATCH_SIZE * OPTIMAL_ACCUMULATION}\")\n",
    "print(f\"   Expected time: 12-15 min/epoch\")\n",
    "print(f\"   Memory safe: ✅ ({memory_per_batch_gb * 3:.1f}GB < 12GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9aaf6ce-088e-4606-ad9f-6c37e5f75f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating full enhanced model...\n",
      "✅ Enhanced DenseNet121 Multi-Label Model created:\n",
      "   Classes: 15\n",
      "   Pretrained: True\n",
      "   Dropout: 0.4\n",
      "   Attention: True\n",
      "   Feature Fusion: True\n",
      "✅ Enhanced Focal Loss: alpha=0.25, gamma=2.5\n",
      "✅ Asymmetric Loss: gamma_neg=4, gamma_pos=1, clip=0.05\n",
      "✅ Enhanced Combined Loss:\n",
      "   Focal: 0.400\n",
      "   BCE: 0.400\n",
      "   Asymmetric: 0.200\n",
      "🚀 Enhanced RTX 3060 Model Configuration Complete!\n",
      "   Model Size: standard\n",
      "   Enhanced Features: Attention + Multi-layer Classifier\n",
      "   Loss: Triple-component (Focal + BCE + Asymmetric)\n",
      "\n",
      "📊 Full Enhanced Model (Speed Optimized):\n",
      "   Total parameters: 8,798,193\n",
      "   Trainable parameters: 8,798,193\n",
      "   Model size: ~35.2 MB\n",
      "   Attention modules: ✅ Enabled\n",
      "   Advanced loss: ✅ Triple-component\n",
      "   CuDNN optimized: ✅ Enabled\n",
      "   Ready for training: ✅\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Full Enhanced Model (FIXED - No JIT optimization)\n",
    "print(\"🔄 Creating full enhanced model...\")\n",
    "\n",
    "# Create full enhanced model (keeping all advanced features)\n",
    "model, criterion = create_enhanced_model_for_rtx3060(\n",
    "    num_classes=15,\n",
    "    class_weights=class_weights,\n",
    "    model_size=\"standard\"  # Standard for speed, still full enhanced\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Enable CuDNN optimization (safe and effective)\n",
    "torch.backends.cudnn.benchmark = True  # This is fine\n",
    "torch.backends.cudnn.deterministic = False  # Allow faster algorithms\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n📊 Full Enhanced Model (Speed Optimized):\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n",
    "print(f\"   Attention modules: ✅ Enabled\")\n",
    "print(f\"   Advanced loss: ✅ Triple-component\")\n",
    "print(f\"   CuDNN optimized: ✅ Enabled\")\n",
    "print(f\"   Ready for training: ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75b6b882-6ac6-47c8-9f47-bf976e2ddda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Setting up progressive training...\n",
      "✅ Progressive training setup complete!\n",
      "   Stage 1: 20 epochs - Fast convergence\n",
      "   Stage 2: 30 epochs - Refinement (if AUC > 0.72)\n",
      "   Stage 3: 50 epochs - Fine-tuning (if AUC > 0.76)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Progressive Training Setup (20→30→50 epochs)\n",
    "print(\"🔄 Setting up progressive training...\")\n",
    "\n",
    "def create_progressive_trainer(stage=\"stage1\", previous_trainer=None):\n",
    "    \"\"\"Create trainer for different stages of progressive training\"\"\"\n",
    "    \n",
    "    if stage == \"stage1\":\n",
    "        # Stage 1: 20 epochs - Focus on rapid convergence\n",
    "        config = {\n",
    "            'learning_rate': 3e-4,           # Higher LR for initial learning\n",
    "            'scheduler_type': 'onecycle',     # Aggressive scheduling\n",
    "            'warmup_epochs': 2,              # Quick warmup\n",
    "            'max_epochs': 20,                # First checkpoint\n",
    "            'patience': 8,                   # Allow exploration\n",
    "            'label_smoothing': 0.05,         # Light smoothing\n",
    "            'accumulation_steps': OPTIMAL_ACCUMULATION,\n",
    "            'mixed_precision': True\n",
    "        }\n",
    "        print(f\"🎯 Stage 1 Config: 20 epochs, aggressive learning\")\n",
    "        \n",
    "    elif stage == \"stage2\":\n",
    "        # Stage 2: +10 epochs (30 total) - Refinement\n",
    "        config = {\n",
    "            'learning_rate': 1e-4,           # Reduced LR for refinement\n",
    "            'scheduler_type': 'cosine',      # Smooth scheduling\n",
    "            'warmup_epochs': 1,              # Minimal warmup\n",
    "            'max_epochs': 10,                # Additional epochs\n",
    "            'patience': 6,                   # Tighter patience\n",
    "            'label_smoothing': 0.1,          # More smoothing\n",
    "            'accumulation_steps': OPTIMAL_ACCUMULATION,\n",
    "            'mixed_precision': True\n",
    "        }\n",
    "        print(f\"🎯 Stage 2 Config: +10 epochs (30 total), refinement\")\n",
    "        \n",
    "    else:  # stage3\n",
    "        # Stage 3: +20 epochs (50 total) - Fine-tuning\n",
    "        config = {\n",
    "            'learning_rate': 5e-5,           # Low LR for fine-tuning\n",
    "            'scheduler_type': 'plateau',     # Conservative scheduling\n",
    "            'warmup_epochs': 0,              # No warmup needed\n",
    "            'max_epochs': 20,                # Final epochs\n",
    "            'patience': 10,                  # High patience for final gains\n",
    "            'label_smoothing': 0.15,         # Heavy smoothing\n",
    "            'accumulation_steps': OPTIMAL_ACCUMULATION,\n",
    "            'mixed_precision': True\n",
    "        }\n",
    "        print(f\"🎯 Stage 3 Config: +20 epochs (50 total), fine-tuning\")\n",
    "    \n",
    "    trainer = EnhancedRTX3060Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        checkpoint_dir=f'../models/checkpoints_{stage}',\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    # Load previous state if continuing\n",
    "    if previous_trainer and stage != \"stage1\":\n",
    "        # Copy training history\n",
    "        trainer.train_losses = previous_trainer.train_losses.copy()\n",
    "        trainer.val_losses = previous_trainer.val_losses.copy()\n",
    "        trainer.val_aucs = previous_trainer.val_aucs.copy()\n",
    "        trainer.val_aps = previous_trainer.val_aps.copy()\n",
    "        trainer.learning_rates = previous_trainer.learning_rates.copy()\n",
    "        trainer.best_val_auc = previous_trainer.best_val_auc\n",
    "        trainer.best_epoch = previous_trainer.best_epoch\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "print(\"✅ Progressive training setup complete!\")\n",
    "print(\"   Stage 1: 20 epochs - Fast convergence\")\n",
    "print(\"   Stage 2: 30 epochs - Refinement (if AUC > 0.72)\")\n",
    "print(\"   Stage 3: 50 epochs - Fine-tuning (if AUC > 0.76)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6d60902-568a-42e2-b14e-65f2c09f9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Stage 1: Full Enhanced Training (20 epochs)\n",
      "================================================================================\n",
      "🎯 Stage 1 Config: 20 epochs, aggressive learning\n",
      "✅ Mixed precision training enabled (FP16)\n",
      "🚀 Enhanced RTX 3060 Trainer initialized:\n",
      "   Device: cuda\n",
      "   Batch size: 8\n",
      "   Effective batch size: 24\n",
      "   Learning rate: 0.0003\n",
      "   Scheduler: onecycle\n",
      "   Warmup epochs: 2\n",
      "   Mixed precision: True\n",
      "   Label smoothing: 0.05\n",
      "🎯 Target for Stage 1: AUC > 0.72 (improvement > +0.03)\n",
      "🚀 Starting enhanced training for 20 epochs\n",
      "   Progressive training: False\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 New best model saved: AUC = 0.6759\n",
      "Epoch   1/20 | Train Loss: 0.1978 | Val Loss: 0.0803 | Val AUC: 0.6759 | Val AP: 0.1423 | LR: 9.39e-05 | Time: 1338.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 New best model saved: AUC = 0.7088\n",
      "Epoch   2/20 | Train Loss: 0.1505 | Val Loss: 0.0768 | Val AUC: 0.7088 | Val AP: 0.2014 | LR: 2.52e-04 | Time: 1320.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 New best model saved: AUC = 0.7189\n",
      "Epoch   3/20 | Train Loss: 0.1466 | Val Loss: 0.1026 | Val AUC: 0.7189 | Val AP: 0.1860 | LR: 4.68e-04 | Time: 1302.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 New best model saved: AUC = 0.7240\n",
      "Epoch   4/20 | Train Loss: 0.1439 | Val Loss: 0.0740 | Val AUC: 0.7240 | Val AP: 0.1988 | LR: 6.84e-04 | Time: 1298.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 New best model saved: AUC = 0.7392\n",
      "Epoch   5/20 | Train Loss: 0.1413 | Val Loss: 0.0744 | Val AUC: 0.7392 | Val AP: 0.2023 | LR: 8.42e-04 | Time: 1318.7s\n",
      "\n",
      "Per-class AUC scores (Epoch 5):\n",
      "  🔶 No Finding............... 0.727\n",
      "  🔶 Atelectasis.............. 0.711\n",
      "  ✅ Cardiomegaly............. 0.843\n",
      "  🔶 Effusion................. 0.795\n",
      "  🔸 Infiltration............. 0.673\n",
      "  🔸 Mass..................... 0.602\n",
      "  🔸 Nodule................... 0.558\n",
      "  🔸 Pneumonia................ 0.656\n",
      "  ✅ Pneumothorax............. 0.801\n",
      "  🔶 Consolidation............ 0.761\n",
      "  ✅ Edema.................... 0.847\n",
      "  ✅ Emphysema................ 0.885\n",
      "  🔸 Fibrosis................. 0.690\n",
      "  🔸 Pleural_Thickening....... 0.677\n",
      "  ✅ Hernia................... 0.861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6/20 | Train Loss: 0.1381 | Val Loss: 0.0679 | Val AUC: 0.7379 | Val AP: 0.2075 | LR: 9.00e-04 | Time: 1272.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 New best model saved: AUC = 0.7406\n",
      "Epoch   7/20 | Train Loss: 0.1360 | Val Loss: 0.0700 | Val AUC: 0.7406 | Val AP: 0.2179 | LR: 8.89e-04 | Time: 1264.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 New best model saved: AUC = 0.7476\n",
      "Epoch   8/20 | Train Loss: 0.1331 | Val Loss: 0.0705 | Val AUC: 0.7476 | Val AP: 0.2191 | LR: 8.55e-04 | Time: 1265.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 New best model saved: AUC = 0.7622\n",
      "Epoch   9/20 | Train Loss: 0.1312 | Val Loss: 0.0668 | Val AUC: 0.7622 | Val AP: 0.2266 | LR: 8.02e-04 | Time: 1264.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/20 | Train Loss: 0.1299 | Val Loss: 0.0695 | Val AUC: 0.7559 | Val AP: 0.2294 | LR: 7.31e-04 | Time: 1250.9s\n",
      "\n",
      "Per-class AUC scores (Epoch 10):\n",
      "  🔶 No Finding............... 0.743\n",
      "  🔶 Atelectasis.............. 0.749\n",
      "  ✅ Cardiomegaly............. 0.908\n",
      "  ✅ Effusion................. 0.831\n",
      "  🔸 Infiltration............. 0.684\n",
      "  🔸 Mass..................... 0.641\n",
      "  🔸 Nodule................... 0.565\n",
      "  🔸 Pneumonia................ 0.687\n",
      "  ✅ Pneumothorax............. 0.829\n",
      "  🔶 Consolidation............ 0.770\n",
      "  ✅ Edema.................... 0.872\n",
      "  ✅ Emphysema................ 0.925\n",
      "  🔶 Fibrosis................. 0.741\n",
      "  🔸 Pleural_Thickening....... 0.682\n",
      "  🔶 Hernia................... 0.714\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11/20 | Train Loss: 0.1279 | Val Loss: 0.0659 | Val AUC: 0.7581 | Val AP: 0.2316 | LR: 6.45e-04 | Time: 1266.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12/20 | Train Loss: 0.1263 | Val Loss: 0.0668 | Val AUC: 0.7569 | Val AP: 0.2331 | LR: 5.50e-04 | Time: 1245.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13/20 | Train Loss: 0.1246 | Val Loss: 0.0651 | Val AUC: 0.7598 | Val AP: 0.2374 | LR: 4.50e-04 | Time: 1264.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 New best model saved: AUC = 0.7675\n",
      "Epoch  14/20 | Train Loss: 0.1224 | Val Loss: 0.0650 | Val AUC: 0.7675 | Val AP: 0.2402 | LR: 3.50e-04 | Time: 1264.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/20 | Train Loss: 0.1205 | Val Loss: 0.0642 | Val AUC: 0.7617 | Val AP: 0.2384 | LR: 2.55e-04 | Time: 1264.6s\n",
      "\n",
      "Per-class AUC scores (Epoch 15):\n",
      "  🔶 No Finding............... 0.747\n",
      "  🔶 Atelectasis.............. 0.744\n",
      "  ✅ Cardiomegaly............. 0.906\n",
      "  ✅ Effusion................. 0.853\n",
      "  🔸 Infiltration............. 0.681\n",
      "  🔸 Mass..................... 0.692\n",
      "  🔸 Nodule................... 0.583\n",
      "  🔸 Pneumonia................ 0.689\n",
      "  ✅ Pneumothorax............. 0.846\n",
      "  🔶 Consolidation............ 0.768\n",
      "  ✅ Edema.................... 0.874\n",
      "  ✅ Emphysema................ 0.924\n",
      "  🔶 Fibrosis................. 0.750\n",
      "  🔸 Pleural_Thickening....... 0.695\n",
      "  🔸 Hernia................... 0.676\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16/20 | Train Loss: 0.1178 | Val Loss: 0.0609 | Val AUC: 0.7648 | Val AP: 0.2377 | LR: 1.70e-04 | Time: 1233.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17/20 | Train Loss: 0.1157 | Val Loss: 0.0615 | Val AUC: 0.7608 | Val AP: 0.2360 | LR: 9.83e-05 | Time: 1234.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18/20 | Train Loss: 0.1141 | Val Loss: 0.0601 | Val AUC: 0.7647 | Val AP: 0.2369 | LR: 4.46e-05 | Time: 1234.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  19/20 | Train Loss: 0.1137 | Val Loss: 0.0604 | Val AUC: 0.7615 | Val AP: 0.2370 | LR: 1.13e-05 | Time: 1234.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/20 | Train Loss: 0.1132 | Val Loss: 0.0592 | Val AUC: 0.7579 | Val AP: 0.2358 | LR: 3.62e-09 | Time: 1246.4s\n",
      "\n",
      "Per-class AUC scores (Epoch 20):\n",
      "  🔶 No Finding............... 0.750\n",
      "  🔶 Atelectasis.............. 0.741\n",
      "  ✅ Cardiomegaly............. 0.902\n",
      "  ✅ Effusion................. 0.855\n",
      "  🔸 Infiltration............. 0.676\n",
      "  🔶 Mass..................... 0.725\n",
      "  🔸 Nodule................... 0.587\n",
      "  🔸 Pneumonia................ 0.648\n",
      "  ✅ Pneumothorax............. 0.841\n",
      "  🔶 Consolidation............ 0.766\n",
      "  ✅ Edema.................... 0.870\n",
      "  ✅ Emphysema................ 0.925\n",
      "  🔶 Fibrosis................. 0.729\n",
      "  🔶 Pleural_Thickening....... 0.706\n",
      "  🔸 Hernia................... 0.647\n",
      "\n",
      "\n",
      "🎉 Enhanced training completed!\n",
      "Total time: 7.05 hours\n",
      "Best validation AUC: 0.7675 (Epoch 14)\n",
      "Final learning rate: 3.62e-09\n",
      "Training stages: 1\n",
      "\n",
      "🏁 Stage 1 Results:\n",
      "==================================================\n",
      "   AUC: 0.7675\n",
      "   Baseline: 0.688\n",
      "   Improvement: +0.0795\n",
      "   Training time: 7.05 hours\n",
      "   Time per epoch: 21.2 minutes\n",
      "✅ Stage 1 SUCCESS! AUC ≥ 0.72\n",
      "   → Proceed to Stage 2 (30 epochs total)\n",
      "💾 Stage 1 model saved: clarity_stage1_auc0.768_ep20.pth\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Stage 1 Training - 20 Epochs\n",
    "print(\"🚀 Starting Stage 1: Full Enhanced Training (20 epochs)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import time\n",
    "stage1_start = time.time()\n",
    "\n",
    "# Create Stage 1 trainer\n",
    "trainer_stage1 = create_progressive_trainer(\"stage1\")\n",
    "\n",
    "# Train Stage 1\n",
    "print(\"🎯 Target for Stage 1: AUC > 0.72 (improvement > +0.03)\")\n",
    "stage1_auc = trainer_stage1.train(num_epochs=20, save_every=5)\n",
    "\n",
    "stage1_time = time.time() - stage1_start\n",
    "\n",
    "print(f\"\\n🏁 Stage 1 Results:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"   AUC: {stage1_auc:.4f}\")\n",
    "print(f\"   Baseline: 0.688\")\n",
    "print(f\"   Improvement: +{stage1_auc - 0.688:.4f}\")\n",
    "print(f\"   Training time: {stage1_time/3600:.2f} hours\")\n",
    "print(f\"   Time per epoch: {stage1_time/60/20:.1f} minutes\")\n",
    "\n",
    "# Decision for Stage 2\n",
    "if stage1_auc >= 0.72:\n",
    "    print(f\"✅ Stage 1 SUCCESS! AUC ≥ 0.72\")\n",
    "    print(f\"   → Proceed to Stage 2 (30 epochs total)\")\n",
    "    proceed_stage2 = True\n",
    "elif stage1_auc >= 0.70:\n",
    "    print(f\"🔶 Stage 1 PARTIAL SUCCESS! 0.70 ≤ AUC < 0.72\")\n",
    "    print(f\"   → Consider Stage 2 for potential gains\")\n",
    "    proceed_stage2 = True\n",
    "else:\n",
    "    print(f\"🔸 Stage 1 needs improvement. AUC < 0.70\")\n",
    "    print(f\"   → May need hyperparameter adjustment\")\n",
    "    proceed_stage2 = False\n",
    "\n",
    "# Save Stage 1 model regardless\n",
    "stage1_model_name = f\"clarity_stage1_auc{stage1_auc:.3f}_ep20.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'stage1_auc': stage1_auc,\n",
    "    'stage': 'stage1_complete',\n",
    "    'epochs': 20,\n",
    "    'training_time_hours': stage1_time / 3600,\n",
    "    'next_stage_recommended': proceed_stage2\n",
    "}, f\"../models/saved_models/{stage1_model_name}\")\n",
    "\n",
    "print(f\"💾 Stage 1 model saved: {stage1_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b62bec63-6f0c-41fd-aea8-a12eefd13406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array lengths: train=20, val=20, auc=20, ap=20, lr=20\n",
      "Using minimum length: 20\n",
      "✅ Model, training history, and related data saved to: ..\\models\\saved_models\\enhanced_densenet121\n",
      "   Epochs saved: 20\n",
      "   Final AUC: 0.7579\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Create save directory\n",
    "save_dir = Path(\"../models/saved_models/enhanced_densenet121\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model weights and architecture info\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': trainer_stage1.optimizer.state_dict(),\n",
    "    'scheduler_state_dict': trainer_stage1.scheduler.state_dict(),\n",
    "    'architecture': 'DenseNet121Enhanced',\n",
    "    'num_classes': 15,\n",
    "    'image_size': getattr(model, 'image_size', 448),\n",
    "    'use_attention': getattr(model, 'use_attention', True),\n",
    "    'feature_fusion': getattr(model, 'feature_fusion', True),\n",
    "    'dropout_rate': 0.4,\n",
    "    'pretrained': True\n",
    "}, save_dir / \"model.pth\")\n",
    "\n",
    "# Fix array length mismatch\n",
    "train_losses = trainer_stage1.train_losses\n",
    "val_losses = trainer_stage1.val_losses\n",
    "val_aucs = trainer_stage1.val_aucs\n",
    "val_aps = trainer_stage1.val_aps\n",
    "learning_rates = trainer_stage1.learning_rates\n",
    "\n",
    "# Find minimum length to ensure all arrays are same size\n",
    "min_length = min(len(train_losses), len(val_losses), len(val_aucs), len(val_aps), len(learning_rates))\n",
    "\n",
    "print(f\"Array lengths: train={len(train_losses)}, val={len(val_losses)}, auc={len(val_aucs)}, ap={len(val_aps)}, lr={len(learning_rates)}\")\n",
    "print(f\"Using minimum length: {min_length}\")\n",
    "\n",
    "# Save training history (CSV) with matched lengths\n",
    "pd.DataFrame({\n",
    "    'epoch': range(1, min_length + 1),\n",
    "    'train_loss': train_losses[:min_length],\n",
    "    'val_loss': val_losses[:min_length],\n",
    "    'val_auc': val_aucs[:min_length],\n",
    "    'val_ap': val_aps[:min_length],\n",
    "    'learning_rate': learning_rates[:min_length]\n",
    "}).to_csv(save_dir / \"training_history.csv\", index=False)\n",
    "\n",
    "# Save key training metrics as numpy file\n",
    "np.savez_compressed(\n",
    "    save_dir / \"training_metrics.npz\",\n",
    "    train_losses=np.array(train_losses),\n",
    "    val_losses=np.array(val_losses),\n",
    "    val_aucs=np.array(val_aucs),\n",
    "    val_aps=np.array(val_aps),\n",
    "    learning_rates=np.array(learning_rates)\n",
    ")\n",
    "\n",
    "print(\"✅ Model, training history, and related data saved to:\", save_dir)\n",
    "print(f\"   Epochs saved: {min_length}\")\n",
    "print(f\"   Final AUC: {val_aucs[-1]:.4f}\" if val_aucs else \"   No AUC data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff2fb6-2f17-4f40-b674-eefa683c94ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
