{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ccc14a6-ec93-4e23-9cdd-c108a82937e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced modules imported successfully\n",
      "üéØ Target: 0.85+ AUC with advanced features\n",
      "üî• Features: Multi-attention, Pyramid pooling, TTA, Mixup/Cutmix\n",
      "üöÄ Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 3060\n",
      "   VRAM: 12.9 GB\n",
      "\n",
      "üéâ CLARITy Enhanced EfficientNet-B4 Training\n",
      "   From: Fast Stable B3 (0.7118 AUC)\n",
      "   To: Enhanced B4 (0.85+ AUC target)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Enhanced Setup and Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import enhanced modules\n",
    "from src.data.dataset import ChestXrayDataset, create_data_splits, calculate_class_weights\n",
    "from src.data.dataloader import create_rtx3060_dataloaders\n",
    "from src.models.enhanced_efficientnet_b4 import (\n",
    "    create_enhanced_efficientnet_b4,\n",
    "    create_enhanced_focal_loss\n",
    ")\n",
    "from src.trainer.enhanced_trainer import EnhancedTrainer\n",
    "\n",
    "print(\"‚úÖ Enhanced modules imported successfully\")\n",
    "print(\"üéØ Target: 0.85+ AUC with advanced features\")\n",
    "print(\"üî• Features: Multi-attention, Pyramid pooling, TTA, Mixup/Cutmix\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(f\"\\nüéâ CLARITy Enhanced EfficientNet-B4 Training\")\n",
    "print(f\"   From: Fast Stable B3 (0.7118 AUC)\")\n",
    "print(f\"   To: Enhanced B4 (0.85+ AUC target)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9993416-6cb8-4216-972c-b2e8ad76d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading data pipeline for enhanced training...\n",
      "‚úÖ Metadata loaded: 112,120 entries\n",
      "‚úÖ Image mapping: 112,120 images\n",
      "Patient-level data splits:\n",
      "  Train: 83,847 images from 23,105 patients (74.8%)\n",
      "  Val:   11,550 images from 3,080 patients (10.3%)\n",
      "  Test:  16,723 images from 4,620 patients (14.9%)\n",
      "‚úÖ No patient overlap verified - clean splits!\n",
      "‚úÖ Data splits: Train(83,847) Val(11,550) Test(16,723)\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "   Total images: 112,120\n",
      "   Training set: 83,847 (74.8%)\n",
      "   Validation set: 11,550 (10.3%)\n",
      "   Test set: 16,723 (14.9%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data Pipeline for Enhanced Training\n",
    "BASE_PATH = Path(r\"D:/Projects/CLARITY/Model/Dataset/archive\")  # Update this path!\n",
    "\n",
    "print(\"üîÑ Loading data pipeline for enhanced training...\")\n",
    "\n",
    "# Load metadata\n",
    "data_entry_path = BASE_PATH / \"Data_Entry_2017.csv\"\n",
    "df = pd.read_csv(data_entry_path)\n",
    "print(f\"‚úÖ Metadata loaded: {len(df):,} entries\")\n",
    "\n",
    "# Create image mapping\n",
    "image_mapping = {}\n",
    "for main_folder in sorted(BASE_PATH.iterdir()):\n",
    "    if main_folder.is_dir() and main_folder.name.startswith('images_'):\n",
    "        images_subfolder = main_folder / 'images'\n",
    "        if images_subfolder.exists():\n",
    "            for img_file in images_subfolder.glob(\"*.png\"):\n",
    "                image_name = img_file.name\n",
    "                if image_name not in image_mapping:\n",
    "                    image_mapping[image_name] = img_file\n",
    "\n",
    "print(f\"‚úÖ Image mapping: {len(image_mapping):,} images\")\n",
    "\n",
    "# Create data splits\n",
    "train_df, val_df, test_df = create_data_splits(df, \n",
    "                                               test_size=0.15,\n",
    "                                               val_size=0.10,\n",
    "                                               random_seed=42)\n",
    "\n",
    "print(f\"‚úÖ Data splits: Train({len(train_df):,}) Val({len(val_df):,}) Test({len(test_df):,})\")\n",
    "\n",
    "# Display data distribution\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Total images: {len(df):,}\")\n",
    "print(f\"   Training set: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Test set: {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9eba5b-dccb-406e-8f05-860413c2431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating enhanced datasets...\n",
      "‚öôÔ∏è  Enhanced Training Configuration:\n",
      "   Image size: 448√ó448\n",
      "   Batch size: 6\n",
      "   Effective batch: 36\n",
      "   Workers: 4\n",
      "   Target: 0.85+ AUC\n",
      "   Expected time per epoch: 20-25 minutes\n",
      "   Expected total time: 20-25 hours\n",
      "Dataset created with 83847 samples\n",
      "Training mode: True\n",
      "Image size: 448x448\n",
      "\n",
      "Label matrix created: (83847, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............  45146\n",
      "  Atelectasis..............   8720\n",
      "  Cardiomegaly.............   2019\n",
      "  Effusion.................  10071\n",
      "  Infiltration.............  14772\n",
      "  Mass.....................   4477\n",
      "  Nodule...................   4691\n",
      "  Pneumonia................   1062\n",
      "  Pneumothorax.............   3981\n",
      "  Consolidation............   3458\n",
      "  Edema....................   1738\n",
      "  Emphysema................   1794\n",
      "  Fibrosis.................   1236\n",
      "  Pleural_Thickening.......   2562\n",
      "  Hernia...................    171\n",
      "Transforms created for training\n",
      "Dataset created with 11550 samples\n",
      "Training mode: False\n",
      "Image size: 448x448\n",
      "\n",
      "Label matrix created: (11550, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............   6197\n",
      "  Atelectasis..............   1148\n",
      "  Cardiomegaly.............    331\n",
      "  Effusion.................   1311\n",
      "  Infiltration.............   2181\n",
      "  Mass.....................    470\n",
      "  Nodule...................    710\n",
      "  Pneumonia................    149\n",
      "  Pneumothorax.............    511\n",
      "  Consolidation............    517\n",
      "  Edema....................    232\n",
      "  Emphysema................    305\n",
      "  Fibrosis.................    169\n",
      "  Pleural_Thickening.......    326\n",
      "  Hernia...................     13\n",
      "Transforms created for validation\n",
      "Dataset created with 16723 samples\n",
      "Training mode: False\n",
      "Image size: 448x448\n",
      "\n",
      "Label matrix created: (16723, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............   9018\n",
      "  Atelectasis..............   1691\n",
      "  Cardiomegaly.............    426\n",
      "  Effusion.................   1935\n",
      "  Infiltration.............   2941\n",
      "  Mass.....................    835\n",
      "  Nodule...................    930\n",
      "  Pneumonia................    220\n",
      "  Pneumothorax.............    810\n",
      "  Consolidation............    692\n",
      "  Edema....................    333\n",
      "  Emphysema................    417\n",
      "  Fibrosis.................    281\n",
      "  Pleural_Thickening.......    497\n",
      "  Hernia...................     43\n",
      "Transforms created for validation\n",
      "‚úÖ Enhanced datasets created!\n",
      "   Training samples: 83,847\n",
      "   Validation samples: 11,550\n",
      "   Test samples: 16,723\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create Enhanced Datasets\n",
    "print(\"üîÑ Creating enhanced datasets...\")\n",
    "\n",
    "# ENHANCED SETTINGS - Optimized for maximum performance\n",
    "ENHANCED_IMAGE_SIZE = 448    # Higher resolution for B4\n",
    "ENHANCED_BATCH_SIZE = 6      # Smaller batch due to model complexity\n",
    "ENHANCED_WORKERS = 4         # Optimized workers\n",
    "ENHANCED_ACCUMULATION = 6    # Effective batch = 36\n",
    "\n",
    "print(f\"‚öôÔ∏è  Enhanced Training Configuration:\")\n",
    "print(f\"   Image size: {ENHANCED_IMAGE_SIZE}√ó{ENHANCED_IMAGE_SIZE}\")\n",
    "print(f\"   Batch size: {ENHANCED_BATCH_SIZE}\")\n",
    "print(f\"   Effective batch: {ENHANCED_BATCH_SIZE * ENHANCED_ACCUMULATION}\")\n",
    "print(f\"   Workers: {ENHANCED_WORKERS}\")\n",
    "print(f\"   Target: 0.85+ AUC\")\n",
    "print(f\"   Expected time per epoch: 20-25 minutes\")\n",
    "print(f\"   Expected total time: 20-25 hours\")\n",
    "\n",
    "# Create enhanced datasets with maximum augmentation\n",
    "train_dataset = ChestXrayDataset(\n",
    "    df=train_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=ENHANCED_IMAGE_SIZE,\n",
    "    is_training=True,\n",
    "    augmentation_prob=0.95  # Maximum augmentation for generalization\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDataset(\n",
    "    df=val_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=ENHANCED_IMAGE_SIZE,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDataset(\n",
    "    df=test_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=ENHANCED_IMAGE_SIZE,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Enhanced datasets created!\")\n",
    "print(f\"   Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   Test samples: {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e61b48-db6f-492b-8433-984d2bf55ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating enhanced dataloaders...\n",
      "\n",
      "Class weights (inverse_freq_sqrt):\n",
      "--------------------------------------------------\n",
      "No Finding...............    1.000 (pos:  45146)\n",
      "Atelectasis..............    1.000 (pos:   8720)\n",
      "Cardiomegaly.............    1.000 (pos:   2019)\n",
      "Effusion.................    1.000 (pos:  10071)\n",
      "Infiltration.............    1.000 (pos:  14772)\n",
      "Mass.....................    1.000 (pos:   4477)\n",
      "Nodule...................    1.000 (pos:   4691)\n",
      "Pneumonia................    1.000 (pos:   1062)\n",
      "Pneumothorax.............    1.000 (pos:   3981)\n",
      "Consolidation............    1.000 (pos:   3458)\n",
      "Edema....................    1.000 (pos:   1738)\n",
      "Emphysema................    1.000 (pos:   1794)\n",
      "Fibrosis.................    1.000 (pos:   1236)\n",
      "Pleural_Thickening.......    1.000 (pos:   2562)\n",
      "Hernia...................    1.000 (pos:    171)\n",
      "üîç Class weight analysis:\n",
      "   No Finding: 45,146.0 samples, weight: 1.000\n",
      "   Atelectasis: 8,720.0 samples, weight: 1.000\n",
      "   Cardiomegaly: 2,019.0 samples, weight: 1.000\n",
      "   Effusion: 10,071.0 samples, weight: 1.000\n",
      "   Infiltration: 14,772.0 samples, weight: 1.000\n",
      "   Mass: 4,477.0 samples, weight: 1.000\n",
      "   Nodule: 4,691.0 samples, weight: 1.000\n",
      "   Pneumonia: 1,062.0 samples, weight: 1.000\n",
      "   Pneumothorax: 3,981.0 samples, weight: 1.000\n",
      "   Consolidation: 3,458.0 samples, weight: 1.000\n",
      "   Edema: 1,738.0 samples, weight: 1.000\n",
      "   Emphysema: 1,794.0 samples, weight: 1.000\n",
      "   Fibrosis: 1,236.0 samples, weight: 1.000\n",
      "   Pleural_Thickening: 2,562.0 samples, weight: 1.000\n",
      "   Hernia: 171.0 samples, weight: 1.000\n",
      "   üî• Hernia boosted: 1.000 ‚Üí 2.000\n",
      "   üî• Pleural_Thickening boosted: 1.000 ‚Üí 2.000\n",
      "   üî• Fibrosis boosted: 1.000 ‚Üí 2.000\n",
      "‚úÖ Enhanced class weights calculated with minority boosting\n",
      "Creating RTX 3060 optimized dataloaders:\n",
      "  Batch size: 6\n",
      "  Num workers: 4\n",
      "  Weighted sampling: True\n",
      "  Weighted sampler created with 83847 samples\n",
      "DataLoaders created:\n",
      "  Train: 13974 batches\n",
      "  Val:   1925 batches\n",
      "  Test:  2788 batches\n",
      "‚úÖ Enhanced dataloaders created!\n",
      "   Train batches: 13974\n",
      "   Val batches: 1925\n",
      "   Test batches: 2788\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Enhanced DataLoaders\n",
    "print(\"üîÑ Creating enhanced dataloaders...\")\n",
    "\n",
    "# Calculate enhanced class weights with minority boosting\n",
    "class_weights = calculate_class_weights(train_dataset.labels, method='inverse_freq_sqrt')\n",
    "\n",
    "# Apply additional minority class boosting\n",
    "minority_indices = [-1, -2, -3]  # Hernia, Pleural_Thickening, Fibrosis\n",
    "original_weights = class_weights.clone()\n",
    "\n",
    "print(\"üîç Class weight analysis:\")\n",
    "disease_classes = [\n",
    "    'No Finding', 'Atelectasis', 'Cardiomegaly', 'Effusion', \n",
    "    'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', \n",
    "    'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', \n",
    "    'Pleural_Thickening', 'Hernia'\n",
    "]\n",
    "\n",
    "for i, disease in enumerate(disease_classes):\n",
    "    count = np.sum(train_dataset.labels[:, i])\n",
    "    weight = class_weights[i].item()\n",
    "    print(f\"   {disease}: {count:,} samples, weight: {weight:.3f}\")\n",
    "\n",
    "# Boost minority classes even more\n",
    "for idx in minority_indices:\n",
    "    class_weights[idx] *= 2.0  # Additional 2x boost\n",
    "    print(f\"   üî• {disease_classes[idx]} boosted: {original_weights[idx].item():.3f} ‚Üí {class_weights[idx].item():.3f}\")\n",
    "\n",
    "print(f\"‚úÖ Enhanced class weights calculated with minority boosting\")\n",
    "\n",
    "# Create enhanced dataloaders\n",
    "train_loader, val_loader, test_loader = create_rtx3060_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=ENHANCED_BATCH_SIZE,\n",
    "    num_workers=ENHANCED_WORKERS,\n",
    "    use_weighted_sampling=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Enhanced dataloaders created!\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ef3187-ff5f-4916-904a-6dd63f1371e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating enhanced EfficientNet-B4 model...\n",
      "‚úÖ Enhanced EfficientNet-B4 Created:\n",
      "   Feature dim: 1792\n",
      "   Advanced attention: ‚úÖ\n",
      "   Pyramid pooling: ‚úÖ\n",
      "   Multi-scale fusion: ‚úÖ\n",
      "   Parameters: 74,286,268\n",
      "‚úÖ Enhanced Focal Loss Created:\n",
      "   Alpha: 0.25\n",
      "   Gamma: 2.5\n",
      "   Minority boost: 5.0x\n",
      "   Label smoothing: 0.1\n",
      "\n",
      "üìä Enhanced Model Statistics:\n",
      "   Total parameters: 74,286,268\n",
      "   Trainable parameters: 74,286,268\n",
      "   Architecture: Enhanced EfficientNet-B4\n",
      "   Input resolution: 448√ó448\n",
      "\n",
      "üî• Advanced Features:\n",
      "   ‚Ä¢ Multi-scale attention ‚úÖ\n",
      "   ‚Ä¢ Pyramid pooling ‚úÖ\n",
      "   ‚Ä¢ Self-attention mechanism ‚úÖ\n",
      "   ‚Ä¢ Enhanced focal loss ‚úÖ\n",
      "   ‚Ä¢ Minority class boosting (5x) ‚úÖ\n",
      "   ‚Ä¢ Label smoothing ‚úÖ\n",
      "\n",
      "üíæ Memory Status:\n",
      "   GPU memory allocated: 0.30 GB\n",
      "   GPU memory cached: 0.31 GB\n",
      "‚úÖ Enhanced model ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Enhanced EfficientNet-B4 Model\n",
    "print(\"üîÑ Creating enhanced EfficientNet-B4 model...\")\n",
    "\n",
    "# Create the most advanced model\n",
    "model = create_enhanced_efficientnet_b4(\n",
    "    num_classes=15,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Create enhanced focal loss with strong minority boosting\n",
    "criterion = create_enhanced_focal_loss(\n",
    "    class_weights=class_weights,\n",
    "    minority_boost=5.0  # 5x boost for minority classes\n",
    ")\n",
    "\n",
    "# Print model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Enhanced Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Architecture: Enhanced EfficientNet-B4\")\n",
    "print(f\"   Input resolution: {ENHANCED_IMAGE_SIZE}√ó{ENHANCED_IMAGE_SIZE}\")\n",
    "\n",
    "print(f\"\\nüî• Advanced Features:\")\n",
    "print(f\"   ‚Ä¢ Multi-scale attention ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Pyramid pooling ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Self-attention mechanism ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Enhanced focal loss ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Minority class boosting (5x) ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Label smoothing ‚úÖ\")\n",
    "\n",
    "# Enable optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Memory check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüíæ Memory Status:\")\n",
    "    print(f\"   GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"   GPU memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "\n",
    "print(\"‚úÖ Enhanced model ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c807911b-afeb-4757-9b8a-99a094217108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing enhanced trainer...\n",
      "‚úÖ Enhanced trainer initialized!\n",
      "\n",
      "‚öôÔ∏è Training Configuration:\n",
      "   Max epochs: 60\n",
      "   Learning rate: 2e-5 (conservative)\n",
      "   Scheduler: OneCycleLR (advanced)\n",
      "   Gradient clipping: 0.5 (strong)\n",
      "   Mixed precision: FP16 ‚úÖ\n",
      "   Mixup/Cutmix: ‚úÖ\n",
      "   Test-time augmentation: ‚úÖ\n",
      "   Early stopping patience: 12 epochs\n",
      "\n",
      "üéØ Performance Targets:\n",
      "   Primary target: 0.85+ AUC\n",
      "   Secondary target: 0.80+ Macro-F1\n",
      "   Expected improvement: +0.13-0.15 AUC over B3\n",
      "\n",
      "‚è±Ô∏è Time Estimates:\n",
      "   Expected time per epoch: 20-25 minutes\n",
      "   Total expected time: 20-25 hours\n",
      "   Early completion possible: ~12-15 hours if target reached\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Initialize Enhanced Trainer\n",
    "print(\"üîÑ Initializing enhanced trainer...\")\n",
    "\n",
    "# Create enhanced trainer with all advanced features\n",
    "trainer = EnhancedTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    \n",
    "    # ENHANCED LEARNING SETTINGS\n",
    "    learning_rate=2e-5,              # Conservative for B4\n",
    "    weight_decay=1e-4,               # Moderate regularization\n",
    "    accumulation_steps=ENHANCED_ACCUMULATION,\n",
    "    mixed_precision=True,            # FP16 for speed\n",
    "    gradient_clipping=0.5,           # Strong clipping for stability\n",
    "    \n",
    "    # ADVANCED FEATURES\n",
    "    scheduler_type='onecycle',       # Advanced scheduling\n",
    "    use_mixup_cutmix=True,          # Data augmentation\n",
    "    test_time_augmentation=True,     # TTA for validation\n",
    "    minority_class_boost=True,       # Special minority handling\n",
    "    \n",
    "    # TRAINING CONFIGURATION\n",
    "    max_epochs=60,                   # Longer training for excellence\n",
    "    patience=12,                     # Patient early stopping\n",
    "    checkpoint_dir='../models/enhanced_checkpoints'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enhanced trainer initialized!\")\n",
    "print(f\"\\n‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"   Max epochs: 60\")\n",
    "print(f\"   Learning rate: 2e-5 (conservative)\")\n",
    "print(f\"   Scheduler: OneCycleLR (advanced)\")\n",
    "print(f\"   Gradient clipping: 0.5 (strong)\")\n",
    "print(f\"   Mixed precision: FP16 ‚úÖ\")\n",
    "print(f\"   Mixup/Cutmix: ‚úÖ\")\n",
    "print(f\"   Test-time augmentation: ‚úÖ\")\n",
    "print(f\"   Early stopping patience: 12 epochs\")\n",
    "\n",
    "print(f\"\\nüéØ Performance Targets:\")\n",
    "print(f\"   Primary target: 0.85+ AUC\")\n",
    "print(f\"   Secondary target: 0.80+ Macro-F1\")\n",
    "print(f\"   Expected improvement: +0.13-0.15 AUC over B3\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Time Estimates:\")\n",
    "print(f\"   Expected time per epoch: 20-25 minutes\")\n",
    "print(f\"   Total expected time: 20-25 hours\")\n",
    "print(f\"   Early completion possible: ~12-15 hours if target reached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf20c6e-e6a7-47c8-8fe7-e6fe0a4ccee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Checking for previous models for transfer learning...\n",
      "üîç Found Fast Stable B3 model - attempting transfer learning...\n",
      "‚úÖ Transfer learning applied successfully!\n",
      "   Compatible layers: 169\n",
      "   Total model layers: 788\n",
      "   Transfer efficiency: 21.4%\n",
      "   Previous B3 AUC: 0.7118\n",
      "   Incompatible layers: 423 (will train from scratch)\n",
      "‚ÑπÔ∏è No DenseNet121 model found\n",
      "\n",
      "‚úÖ Model initialization complete - ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load Previous Models (Optional Transfer Learning)\n",
    "print(\"üîÑ Checking for previous models for transfer learning...\")\n",
    "\n",
    "# Option 1: Load from B3 fast/stable model\n",
    "try:\n",
    "    b3_checkpoint_path = \"../models/fast_stable_checkpoints/best_fast_stable_model.pth\"\n",
    "    if Path(b3_checkpoint_path).exists():\n",
    "        print(\"üîç Found Fast Stable B3 model - attempting transfer learning...\")\n",
    "        \n",
    "        b3_checkpoint = torch.load(b3_checkpoint_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # Extract compatible weights\n",
    "        b3_dict = b3_checkpoint.get('model_state_dict', {})\n",
    "        model_dict = model.state_dict()\n",
    "        \n",
    "        # Filter compatible weights (mainly backbone layers)\n",
    "        compatible_dict = {}\n",
    "        incompatible_layers = []\n",
    "        \n",
    "        for k, v in b3_dict.items():\n",
    "            if k in model_dict:\n",
    "                if v.size() == model_dict[k].size():\n",
    "                    compatible_dict[k] = v\n",
    "                else:\n",
    "                    incompatible_layers.append(k)\n",
    "            else:\n",
    "                incompatible_layers.append(k)\n",
    "        \n",
    "        if compatible_dict:\n",
    "            model_dict.update(compatible_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "            \n",
    "            print(f\"‚úÖ Transfer learning applied successfully!\")\n",
    "            print(f\"   Compatible layers: {len(compatible_dict)}\")\n",
    "            print(f\"   Total model layers: {len(model_dict)}\")\n",
    "            print(f\"   Transfer efficiency: {len(compatible_dict)/len(model_dict)*100:.1f}%\")\n",
    "            print(f\"   Previous B3 AUC: {b3_checkpoint.get('best_val_auc', 0.7118):.4f}\")\n",
    "            \n",
    "            if incompatible_layers:\n",
    "                print(f\"   Incompatible layers: {len(incompatible_layers)} (will train from scratch)\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No compatible layers found - starting from ImageNet pretrained\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è B3 checkpoint not found - starting from ImageNet pretrained\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Transfer learning failed: {e}\")\n",
    "    print(\"Starting from ImageNet pretrained weights...\")\n",
    "\n",
    "# Option 2: Load from DenseNet121 (if available)\n",
    "try:\n",
    "    densenet_path = \"../models/saved_models/enhanced_densenet121/enhanced_densenet121.pth\"\n",
    "    if Path(densenet_path).exists():\n",
    "        print(\"üîç DenseNet121 model found but skipping (different architecture)\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No DenseNet121 model found\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n‚úÖ Model initialization complete - ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a690b713-4774-4fdb-b870-88e08b95c852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Starting Enhanced EfficientNet-B4 Training...\n",
      "   From: Fast Stable B3 (0.7118 AUC)\n",
      "   To: Enhanced B4 (0.85+ AUC target)\n",
      "   Features: Multi-attention, TTA, Mixup/Cutmix, Pyramid pooling\n",
      "   Expected time: 20-25 hours\n",
      "====================================================================================================\n",
      "üñ•Ô∏è Pre-training GPU memory:\n",
      "   Allocated: 0.45 GB\n",
      "   Cached: 0.47 GB\n",
      "\n",
      "üöÄ Training started at: 2025-10-05 15:06:53\n",
      "üéØ Starting Enhanced Training for Excellence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∏Ô∏è Training interrupted by user\n",
      "\n",
      "üèÜ ENHANCED TRAINING SESSION COMPLETED!\n",
      "======================================================================\n",
      "üéØ Final Results:\n",
      "   Best AUC: 0.0000\n",
      "   Best Macro-F1: 0.0000\n",
      "   Training time: 0.11 hours\n",
      "   Completed epochs: 0\n",
      "\n",
      "üìä Performance Assessment:\n",
      "‚ö†Ô∏è NEEDS IMPROVEMENT! Consider longer training or ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cell 8: Start Enhanced Training\n",
    "print(\"üéØ Starting Enhanced EfficientNet-B4 Training...\")\n",
    "print(\"   From: Fast Stable B3 (0.7118 AUC)\")\n",
    "print(\"   To: Enhanced B4 (0.85+ AUC target)\")\n",
    "print(\"   Features: Multi-attention, TTA, Mixup/Cutmix, Pyramid pooling\")\n",
    "print(\"   Expected time: 20-25 hours\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Clear CUDA cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Memory monitoring\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üñ•Ô∏è Pre-training GPU memory:\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"   Cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüöÄ Training started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Run enhanced training\n",
    "try:\n",
    "    best_auc, best_f1 = trainer.train_for_excellence()\n",
    "    training_completed = True\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚è∏Ô∏è Training interrupted by user\")\n",
    "    best_auc = trainer.best_val_auc\n",
    "    best_f1 = trainer.best_val_f1\n",
    "    training_completed = False\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    best_auc = trainer.best_val_auc\n",
    "    best_f1 = trainer.best_val_f1\n",
    "    training_completed = False\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüèÜ ENHANCED TRAINING SESSION COMPLETED!\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"üéØ Final Results:\")\n",
    "print(f\"   Best AUC: {best_auc:.4f}\")\n",
    "print(f\"   Best Macro-F1: {best_f1:.4f}\")\n",
    "print(f\"   Training time: {training_time/3600:.2f} hours\")\n",
    "print(f\"   Completed epochs: {len(trainer.val_aucs) if hasattr(trainer, 'val_aucs') else 'N/A'}\")\n",
    "if hasattr(trainer, 'val_aucs') and trainer.val_aucs:\n",
    "    print(f\"   Average time per epoch: {training_time/len(trainer.val_aucs)/60:.1f} minutes\")\n",
    "\n",
    "# Performance assessment\n",
    "print(f\"\\nüìä Performance Assessment:\")\n",
    "if best_auc >= 0.85:\n",
    "    print(f\"‚úÖ EXCELLENCE ACHIEVED! Target 0.85+ AUC reached!\")\n",
    "    print(f\"üéâ Ready for clinical deployment consideration\")\n",
    "    assessment = \"EXCELLENCE\"\n",
    "elif best_auc >= 0.82:\n",
    "    print(f\"üî• HIGH PERFORMANCE! Very close to excellence\")\n",
    "    print(f\"üí° Consider ensemble or fine-tuning for 0.85+\")\n",
    "    assessment = \"HIGH_PERFORMANCE\"\n",
    "elif best_auc >= 0.80:\n",
    "    print(f\"üìà GREAT PROGRESS! Significant improvement\")\n",
    "    print(f\"üí° Ready for ensemble combination\")\n",
    "    assessment = \"GREAT_PROGRESS\"\n",
    "elif best_auc >= 0.75:\n",
    "    print(f\"üìä GOOD FOUNDATION! Solid improvement achieved\")\n",
    "    print(f\"üí° Consider ensemble approach\")\n",
    "    assessment = \"GOOD_FOUNDATION\"\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è NEEDS IMPROVEMENT! Consider longer training or ensemble\")\n",
    "    assessment = \"NEEDS_IMPROVEMENT\"\n",
    "\n",
    "# Improvement calculation\n",
    "if best_auc > 0.7118:  # B3 baseline\n",
    "    improvement = best_auc - 0.7118\n",
    "    print(f\"\\nüìà Improvement over Fast Stable B3:\")\n",
    "    print(f\"   AUC improvement: +{improvement:.4f}\")\n",
    "    print(f\"   Relative improvement: +{improvement/0.7118*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f8073-6664-478f-9a69-4243b82b2e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
