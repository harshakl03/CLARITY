{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ccc14a6-ec93-4e23-9cdd-c108a82937e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enhanced modules imported successfully\n",
      "ðŸŽ¯ Target: 0.85+ AUC with advanced features\n",
      "ðŸ”¥ Features: Multi-attention, Pyramid pooling, TTA, Mixup/Cutmix\n",
      "ðŸš€ Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 3060\n",
      "   VRAM: 12.9 GB\n",
      "\n",
      "ðŸŽ‰ CLARITy Enhanced EfficientNet-B4 Training\n",
      "   From: Fast Stable B3 (0.7118 AUC)\n",
      "   To: Enhanced B4 (0.85+ AUC target)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Enhanced Setup and Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import enhanced modules\n",
    "from src.data.dataset import ChestXrayDataset, create_data_splits, calculate_class_weights\n",
    "from src.data.dataloader import create_rtx3060_dataloaders\n",
    "from src.models.enhanced_efficientnet_b4 import (\n",
    "    create_enhanced_efficientnet_b4,\n",
    "    create_enhanced_focal_loss\n",
    ")\n",
    "from src.trainer.enhanced_trainer import EnhancedTrainer\n",
    "\n",
    "print(\"âœ… Enhanced modules imported successfully\")\n",
    "print(\"ðŸŽ¯ Target: 0.85+ AUC with advanced features\")\n",
    "print(\"ðŸ”¥ Features: Multi-attention, Pyramid pooling, TTA, Mixup/Cutmix\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ CLARITy Enhanced EfficientNet-B4 Training\")\n",
    "print(f\"   From: Fast Stable B3 (0.7118 AUC)\")\n",
    "print(f\"   To: Enhanced B4 (0.85+ AUC target)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9993416-6cb8-4216-972c-b2e8ad76d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading data pipeline for enhanced training...\n",
      "âœ… Metadata loaded: 112,120 entries\n",
      "âœ… Image mapping: 112,120 images\n",
      "Patient-level data splits:\n",
      "  Train: 83,847 images from 23,105 patients (74.8%)\n",
      "  Val:   11,550 images from 3,080 patients (10.3%)\n",
      "  Test:  16,723 images from 4,620 patients (14.9%)\n",
      "âœ… No patient overlap verified - clean splits!\n",
      "âœ… Data splits: Train(83,847) Val(11,550) Test(16,723)\n",
      "\n",
      "ðŸ“Š Dataset Statistics:\n",
      "   Total images: 112,120\n",
      "   Training set: 83,847 (74.8%)\n",
      "   Validation set: 11,550 (10.3%)\n",
      "   Test set: 16,723 (14.9%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data Pipeline for Enhanced Training\n",
    "BASE_PATH = Path(r\"D:/Projects/CLARITY/Model/Dataset/archive\")  # Update this path!\n",
    "\n",
    "print(\"ðŸ”„ Loading data pipeline for enhanced training...\")\n",
    "\n",
    "# Load metadata\n",
    "data_entry_path = BASE_PATH / \"Data_Entry_2017.csv\"\n",
    "df = pd.read_csv(data_entry_path)\n",
    "print(f\"âœ… Metadata loaded: {len(df):,} entries\")\n",
    "\n",
    "# Create image mapping\n",
    "image_mapping = {}\n",
    "for main_folder in sorted(BASE_PATH.iterdir()):\n",
    "    if main_folder.is_dir() and main_folder.name.startswith('images_'):\n",
    "        images_subfolder = main_folder / 'images'\n",
    "        if images_subfolder.exists():\n",
    "            for img_file in images_subfolder.glob(\"*.png\"):\n",
    "                image_name = img_file.name\n",
    "                if image_name not in image_mapping:\n",
    "                    image_mapping[image_name] = img_file\n",
    "\n",
    "print(f\"âœ… Image mapping: {len(image_mapping):,} images\")\n",
    "\n",
    "# Create data splits\n",
    "train_df, val_df, test_df = create_data_splits(df, \n",
    "                                               test_size=0.15,\n",
    "                                               val_size=0.10,\n",
    "                                               random_seed=42)\n",
    "\n",
    "print(f\"âœ… Data splits: Train({len(train_df):,}) Val({len(val_df):,}) Test({len(test_df):,})\")\n",
    "\n",
    "# Display data distribution\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Total images: {len(df):,}\")\n",
    "print(f\"   Training set: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Test set: {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9eba5b-dccb-406e-8f05-860413c2431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating enhanced datasets...\n",
      "âš™ï¸  Enhanced Training Configuration:\n",
      "   Image size: 448Ã—448\n",
      "   Batch size: 6\n",
      "   Effective batch: 36\n",
      "   Workers: 4\n",
      "   Target: 0.85+ AUC\n",
      "   Expected time per epoch: 20-25 minutes\n",
      "   Expected total time: 20-25 hours\n",
      "Dataset created with 83847 samples\n",
      "Training mode: True\n",
      "Image size: 448x448\n",
      "\n",
      "Label matrix created: (83847, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............  45146\n",
      "  Atelectasis..............   8720\n",
      "  Cardiomegaly.............   2019\n",
      "  Effusion.................  10071\n",
      "  Infiltration.............  14772\n",
      "  Mass.....................   4477\n",
      "  Nodule...................   4691\n",
      "  Pneumonia................   1062\n",
      "  Pneumothorax.............   3981\n",
      "  Consolidation............   3458\n",
      "  Edema....................   1738\n",
      "  Emphysema................   1794\n",
      "  Fibrosis.................   1236\n",
      "  Pleural_Thickening.......   2562\n",
      "  Hernia...................    171\n",
      "Transforms created for training\n",
      "Dataset created with 11550 samples\n",
      "Training mode: False\n",
      "Image size: 448x448\n",
      "\n",
      "Label matrix created: (11550, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............   6197\n",
      "  Atelectasis..............   1148\n",
      "  Cardiomegaly.............    331\n",
      "  Effusion.................   1311\n",
      "  Infiltration.............   2181\n",
      "  Mass.....................    470\n",
      "  Nodule...................    710\n",
      "  Pneumonia................    149\n",
      "  Pneumothorax.............    511\n",
      "  Consolidation............    517\n",
      "  Edema....................    232\n",
      "  Emphysema................    305\n",
      "  Fibrosis.................    169\n",
      "  Pleural_Thickening.......    326\n",
      "  Hernia...................     13\n",
      "Transforms created for validation\n",
      "Dataset created with 16723 samples\n",
      "Training mode: False\n",
      "Image size: 448x448\n",
      "\n",
      "Label matrix created: (16723, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............   9018\n",
      "  Atelectasis..............   1691\n",
      "  Cardiomegaly.............    426\n",
      "  Effusion.................   1935\n",
      "  Infiltration.............   2941\n",
      "  Mass.....................    835\n",
      "  Nodule...................    930\n",
      "  Pneumonia................    220\n",
      "  Pneumothorax.............    810\n",
      "  Consolidation............    692\n",
      "  Edema....................    333\n",
      "  Emphysema................    417\n",
      "  Fibrosis.................    281\n",
      "  Pleural_Thickening.......    497\n",
      "  Hernia...................     43\n",
      "Transforms created for validation\n",
      "âœ… Enhanced datasets created!\n",
      "   Training samples: 83,847\n",
      "   Validation samples: 11,550\n",
      "   Test samples: 16,723\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create Enhanced Datasets\n",
    "print(\"ðŸ”„ Creating enhanced datasets...\")\n",
    "\n",
    "# ENHANCED SETTINGS - Optimized for maximum performance\n",
    "ENHANCED_IMAGE_SIZE = 448    # Higher resolution for B4\n",
    "ENHANCED_BATCH_SIZE = 6      # Smaller batch due to model complexity\n",
    "ENHANCED_WORKERS = 4         # Optimized workers\n",
    "ENHANCED_ACCUMULATION = 6    # Effective batch = 36\n",
    "\n",
    "print(f\"âš™ï¸  Enhanced Training Configuration:\")\n",
    "print(f\"   Image size: {ENHANCED_IMAGE_SIZE}Ã—{ENHANCED_IMAGE_SIZE}\")\n",
    "print(f\"   Batch size: {ENHANCED_BATCH_SIZE}\")\n",
    "print(f\"   Effective batch: {ENHANCED_BATCH_SIZE * ENHANCED_ACCUMULATION}\")\n",
    "print(f\"   Workers: {ENHANCED_WORKERS}\")\n",
    "print(f\"   Target: 0.85+ AUC\")\n",
    "print(f\"   Expected time per epoch: 20-25 minutes\")\n",
    "print(f\"   Expected total time: 20-25 hours\")\n",
    "\n",
    "# Create enhanced datasets with maximum augmentation\n",
    "train_dataset = ChestXrayDataset(\n",
    "    df=train_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=ENHANCED_IMAGE_SIZE,\n",
    "    is_training=True,\n",
    "    augmentation_prob=0.95  # Maximum augmentation for generalization\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDataset(\n",
    "    df=val_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=ENHANCED_IMAGE_SIZE,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDataset(\n",
    "    df=test_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=ENHANCED_IMAGE_SIZE,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Enhanced datasets created!\")\n",
    "print(f\"   Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   Test samples: {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e61b48-db6f-492b-8433-984d2bf55ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating enhanced dataloaders...\n",
      "\n",
      "Class weights (inverse_freq_sqrt):\n",
      "--------------------------------------------------\n",
      "No Finding...............    1.000 (pos:  45146)\n",
      "Atelectasis..............    1.000 (pos:   8720)\n",
      "Cardiomegaly.............    1.000 (pos:   2019)\n",
      "Effusion.................    1.000 (pos:  10071)\n",
      "Infiltration.............    1.000 (pos:  14772)\n",
      "Mass.....................    1.000 (pos:   4477)\n",
      "Nodule...................    1.000 (pos:   4691)\n",
      "Pneumonia................    1.000 (pos:   1062)\n",
      "Pneumothorax.............    1.000 (pos:   3981)\n",
      "Consolidation............    1.000 (pos:   3458)\n",
      "Edema....................    1.000 (pos:   1738)\n",
      "Emphysema................    1.000 (pos:   1794)\n",
      "Fibrosis.................    1.000 (pos:   1236)\n",
      "Pleural_Thickening.......    1.000 (pos:   2562)\n",
      "Hernia...................    1.000 (pos:    171)\n",
      "ðŸ” Class weight analysis:\n",
      "   No Finding: 45,146.0 samples, weight: 1.000\n",
      "   Atelectasis: 8,720.0 samples, weight: 1.000\n",
      "   Cardiomegaly: 2,019.0 samples, weight: 1.000\n",
      "   Effusion: 10,071.0 samples, weight: 1.000\n",
      "   Infiltration: 14,772.0 samples, weight: 1.000\n",
      "   Mass: 4,477.0 samples, weight: 1.000\n",
      "   Nodule: 4,691.0 samples, weight: 1.000\n",
      "   Pneumonia: 1,062.0 samples, weight: 1.000\n",
      "   Pneumothorax: 3,981.0 samples, weight: 1.000\n",
      "   Consolidation: 3,458.0 samples, weight: 1.000\n",
      "   Edema: 1,738.0 samples, weight: 1.000\n",
      "   Emphysema: 1,794.0 samples, weight: 1.000\n",
      "   Fibrosis: 1,236.0 samples, weight: 1.000\n",
      "   Pleural_Thickening: 2,562.0 samples, weight: 1.000\n",
      "   Hernia: 171.0 samples, weight: 1.000\n",
      "   ðŸ”¥ Hernia boosted: 1.000 â†’ 2.000\n",
      "   ðŸ”¥ Pleural_Thickening boosted: 1.000 â†’ 2.000\n",
      "   ðŸ”¥ Fibrosis boosted: 1.000 â†’ 2.000\n",
      "âœ… Enhanced class weights calculated with minority boosting\n",
      "Creating RTX 3060 optimized dataloaders:\n",
      "  Batch size: 6\n",
      "  Num workers: 4\n",
      "  Weighted sampling: True\n",
      "  Weighted sampler created with 83847 samples\n",
      "DataLoaders created:\n",
      "  Train: 13974 batches\n",
      "  Val:   1925 batches\n",
      "  Test:  2788 batches\n",
      "âœ… Enhanced dataloaders created!\n",
      "   Train batches: 13974\n",
      "   Val batches: 1925\n",
      "   Test batches: 2788\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Enhanced DataLoaders\n",
    "print(\"ðŸ”„ Creating enhanced dataloaders...\")\n",
    "\n",
    "# Calculate enhanced class weights with minority boosting\n",
    "class_weights = calculate_class_weights(train_dataset.labels, method='inverse_freq_sqrt')\n",
    "\n",
    "# Apply additional minority class boosting\n",
    "minority_indices = [-1, -2, -3]  # Hernia, Pleural_Thickening, Fibrosis\n",
    "original_weights = class_weights.clone()\n",
    "\n",
    "print(\"ðŸ” Class weight analysis:\")\n",
    "disease_classes = [\n",
    "    'No Finding', 'Atelectasis', 'Cardiomegaly', 'Effusion', \n",
    "    'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', \n",
    "    'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', \n",
    "    'Pleural_Thickening', 'Hernia'\n",
    "]\n",
    "\n",
    "for i, disease in enumerate(disease_classes):\n",
    "    count = np.sum(train_dataset.labels[:, i])\n",
    "    weight = class_weights[i].item()\n",
    "    print(f\"   {disease}: {count:,} samples, weight: {weight:.3f}\")\n",
    "\n",
    "# Boost minority classes even more\n",
    "for idx in minority_indices:\n",
    "    class_weights[idx] *= 2.0  # Additional 2x boost\n",
    "    print(f\"   ðŸ”¥ {disease_classes[idx]} boosted: {original_weights[idx].item():.3f} â†’ {class_weights[idx].item():.3f}\")\n",
    "\n",
    "print(f\"âœ… Enhanced class weights calculated with minority boosting\")\n",
    "\n",
    "# Create enhanced dataloaders\n",
    "train_loader, val_loader, test_loader = create_rtx3060_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=ENHANCED_BATCH_SIZE,\n",
    "    num_workers=ENHANCED_WORKERS,\n",
    "    use_weighted_sampling=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Enhanced dataloaders created!\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ef3187-ff5f-4916-904a-6dd63f1371e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating enhanced EfficientNet-B4 model...\n",
      "âœ… Enhanced EfficientNet-B4 Created:\n",
      "   Feature dim: 1792\n",
      "   Advanced attention: âœ…\n",
      "   Pyramid pooling: âœ…\n",
      "   Multi-scale fusion: âœ…\n",
      "   Parameters: 74,286,268\n",
      "âœ… Enhanced Focal Loss Created:\n",
      "   Alpha: 0.25\n",
      "   Gamma: 2.5\n",
      "   Minority boost: 5.0x\n",
      "   Label smoothing: 0.1\n",
      "\n",
      "ðŸ“Š Enhanced Model Statistics:\n",
      "   Total parameters: 74,286,268\n",
      "   Trainable parameters: 74,286,268\n",
      "   Architecture: Enhanced EfficientNet-B4\n",
      "   Input resolution: 448Ã—448\n",
      "\n",
      "ðŸ”¥ Advanced Features:\n",
      "   â€¢ Multi-scale attention âœ…\n",
      "   â€¢ Pyramid pooling âœ…\n",
      "   â€¢ Self-attention mechanism âœ…\n",
      "   â€¢ Enhanced focal loss âœ…\n",
      "   â€¢ Minority class boosting (5x) âœ…\n",
      "   â€¢ Label smoothing âœ…\n",
      "\n",
      "ðŸ’¾ Memory Status:\n",
      "   GPU memory allocated: 0.30 GB\n",
      "   GPU memory cached: 0.31 GB\n",
      "âœ… Enhanced model ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Enhanced EfficientNet-B4 Model\n",
    "print(\"ðŸ”„ Creating enhanced EfficientNet-B4 model...\")\n",
    "\n",
    "# Create the most advanced model\n",
    "model = create_enhanced_efficientnet_b4(\n",
    "    num_classes=15,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Create enhanced focal loss with strong minority boosting\n",
    "criterion = create_enhanced_focal_loss(\n",
    "    class_weights=class_weights,\n",
    "    minority_boost=5.0  # 5x boost for minority classes\n",
    ")\n",
    "\n",
    "# Print model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nðŸ“Š Enhanced Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Architecture: Enhanced EfficientNet-B4\")\n",
    "print(f\"   Input resolution: {ENHANCED_IMAGE_SIZE}Ã—{ENHANCED_IMAGE_SIZE}\")\n",
    "\n",
    "print(f\"\\nðŸ”¥ Advanced Features:\")\n",
    "print(f\"   â€¢ Multi-scale attention âœ…\")\n",
    "print(f\"   â€¢ Pyramid pooling âœ…\")\n",
    "print(f\"   â€¢ Self-attention mechanism âœ…\")\n",
    "print(f\"   â€¢ Enhanced focal loss âœ…\")\n",
    "print(f\"   â€¢ Minority class boosting (5x) âœ…\")\n",
    "print(f\"   â€¢ Label smoothing âœ…\")\n",
    "\n",
    "# Enable optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Memory check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nðŸ’¾ Memory Status:\")\n",
    "    print(f\"   GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"   GPU memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "\n",
    "print(\"âœ… Enhanced model ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c807911b-afeb-4757-9b8a-99a094217108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Initializing enhanced trainer...\n",
      "âœ… Enhanced trainer initialized!\n",
      "\n",
      "âš™ï¸ Training Configuration:\n",
      "   Max epochs: 60\n",
      "   Learning rate: 2e-5 (conservative)\n",
      "   Scheduler: OneCycleLR (advanced)\n",
      "   Gradient clipping: 0.5 (strong)\n",
      "   Mixed precision: FP16 âœ…\n",
      "   Mixup/Cutmix: âœ…\n",
      "   Test-time augmentation: âœ…\n",
      "   Early stopping patience: 12 epochs\n",
      "\n",
      "ðŸŽ¯ Performance Targets:\n",
      "   Primary target: 0.85+ AUC\n",
      "   Secondary target: 0.80+ Macro-F1\n",
      "   Expected improvement: +0.13-0.15 AUC over B3\n",
      "\n",
      "â±ï¸ Time Estimates:\n",
      "   Expected time per epoch: 20-25 minutes\n",
      "   Total expected time: 20-25 hours\n",
      "   Early completion possible: ~12-15 hours if target reached\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Initialize Enhanced Trainer\n",
    "print(\"ðŸ”„ Initializing enhanced trainer...\")\n",
    "\n",
    "# Create enhanced trainer with all advanced features\n",
    "trainer = EnhancedTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    \n",
    "    # ENHANCED LEARNING SETTINGS\n",
    "    learning_rate=2e-5,              # Conservative for B4\n",
    "    weight_decay=1e-4,               # Moderate regularization\n",
    "    accumulation_steps=ENHANCED_ACCUMULATION,\n",
    "    mixed_precision=True,            # FP16 for speed\n",
    "    gradient_clipping=0.5,           # Strong clipping for stability\n",
    "    \n",
    "    # ADVANCED FEATURES\n",
    "    scheduler_type='onecycle',       # Advanced scheduling\n",
    "    use_mixup_cutmix=True,          # Data augmentation\n",
    "    test_time_augmentation=True,     # TTA for validation\n",
    "    minority_class_boost=True,       # Special minority handling\n",
    "    \n",
    "    # TRAINING CONFIGURATION\n",
    "    max_epochs=60,                   # Longer training for excellence\n",
    "    patience=12,                     # Patient early stopping\n",
    "    checkpoint_dir='../models/enhanced_checkpoints'\n",
    ")\n",
    "\n",
    "print(\"âœ… Enhanced trainer initialized!\")\n",
    "print(f\"\\nâš™ï¸ Training Configuration:\")\n",
    "print(f\"   Max epochs: 60\")\n",
    "print(f\"   Learning rate: 2e-5 (conservative)\")\n",
    "print(f\"   Scheduler: OneCycleLR (advanced)\")\n",
    "print(f\"   Gradient clipping: 0.5 (strong)\")\n",
    "print(f\"   Mixed precision: FP16 âœ…\")\n",
    "print(f\"   Mixup/Cutmix: âœ…\")\n",
    "print(f\"   Test-time augmentation: âœ…\")\n",
    "print(f\"   Early stopping patience: 12 epochs\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Performance Targets:\")\n",
    "print(f\"   Primary target: 0.85+ AUC\")\n",
    "print(f\"   Secondary target: 0.80+ Macro-F1\")\n",
    "print(f\"   Expected improvement: +0.13-0.15 AUC over B3\")\n",
    "\n",
    "print(f\"\\nâ±ï¸ Time Estimates:\")\n",
    "print(f\"   Expected time per epoch: 20-25 minutes\")\n",
    "print(f\"   Total expected time: 20-25 hours\")\n",
    "print(f\"   Early completion possible: ~12-15 hours if target reached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf20c6e-e6a7-47c8-8fe7-e6fe0a4ccee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Checking for previous models for transfer learning...\n",
      "ðŸ” Found Fast Stable B3 model - attempting transfer learning...\n",
      "âœ… Transfer learning applied successfully!\n",
      "   Compatible layers: 169\n",
      "   Total model layers: 788\n",
      "   Transfer efficiency: 21.4%\n",
      "   Previous B3 AUC: 0.7118\n",
      "   Incompatible layers: 423 (will train from scratch)\n",
      "â„¹ï¸ No DenseNet121 model found\n",
      "\n",
      "âœ… Model initialization complete - ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load Previous Models (Optional Transfer Learning)\n",
    "print(\"ðŸ”„ Checking for previous models for transfer learning...\")\n",
    "\n",
    "# Option 1: Load from B3 fast/stable model\n",
    "try:\n",
    "    b3_checkpoint_path = \"../models/fast_stable_checkpoints/best_fast_stable_model.pth\"\n",
    "    if Path(b3_checkpoint_path).exists():\n",
    "        print(\"ðŸ” Found Fast Stable B3 model - attempting transfer learning...\")\n",
    "        \n",
    "        b3_checkpoint = torch.load(b3_checkpoint_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # Extract compatible weights\n",
    "        b3_dict = b3_checkpoint.get('model_state_dict', {})\n",
    "        model_dict = model.state_dict()\n",
    "        \n",
    "        # Filter compatible weights (mainly backbone layers)\n",
    "        compatible_dict = {}\n",
    "        incompatible_layers = []\n",
    "        \n",
    "        for k, v in b3_dict.items():\n",
    "            if k in model_dict:\n",
    "                if v.size() == model_dict[k].size():\n",
    "                    compatible_dict[k] = v\n",
    "                else:\n",
    "                    incompatible_layers.append(k)\n",
    "            else:\n",
    "                incompatible_layers.append(k)\n",
    "        \n",
    "        if compatible_dict:\n",
    "            model_dict.update(compatible_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "            \n",
    "            print(f\"âœ… Transfer learning applied successfully!\")\n",
    "            print(f\"   Compatible layers: {len(compatible_dict)}\")\n",
    "            print(f\"   Total model layers: {len(model_dict)}\")\n",
    "            print(f\"   Transfer efficiency: {len(compatible_dict)/len(model_dict)*100:.1f}%\")\n",
    "            print(f\"   Previous B3 AUC: {b3_checkpoint.get('best_val_auc', 0.7118):.4f}\")\n",
    "            \n",
    "            if incompatible_layers:\n",
    "                print(f\"   Incompatible layers: {len(incompatible_layers)} (will train from scratch)\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No compatible layers found - starting from ImageNet pretrained\")\n",
    "    else:\n",
    "        print(\"âš ï¸ B3 checkpoint not found - starting from ImageNet pretrained\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Transfer learning failed: {e}\")\n",
    "    print(\"Starting from ImageNet pretrained weights...\")\n",
    "\n",
    "# Option 2: Load from DenseNet121 (if available)\n",
    "try:\n",
    "    densenet_path = \"../models/saved_models/enhanced_densenet121/enhanced_densenet121.pth\"\n",
    "    if Path(densenet_path).exists():\n",
    "        print(\"ðŸ” DenseNet121 model found but skipping (different architecture)\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ No DenseNet121 model found\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nâœ… Model initialization complete - ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a690b713-4774-4fdb-b870-88e08b95c852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Starting Enhanced EfficientNet-B4 Training...\n",
      "   From: Fast Stable B3 (0.7118 AUC)\n",
      "   To: Enhanced B4 (0.85+ AUC target)\n",
      "   Features: Multi-attention, TTA, Mixup/Cutmix, Pyramid pooling\n",
      "   Expected time: 20-25 hours\n",
      "====================================================================================================\n",
      "ðŸ–¥ï¸ Pre-training GPU memory:\n",
      "   Allocated: 0.45 GB\n",
      "   Cached: 0.47 GB\n",
      "\n",
      "ðŸš€ Training started at: 2025-10-05 15:06:53\n",
      "ðŸŽ¯ Starting Enhanced Training for Excellence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â¸ï¸ Training interrupted by user\n",
      "\n",
      "ðŸ† ENHANCED TRAINING SESSION COMPLETED!\n",
      "======================================================================\n",
      "ðŸŽ¯ Final Results:\n",
      "   Best AUC: 0.0000\n",
      "   Best Macro-F1: 0.0000\n",
      "   Training time: 0.11 hours\n",
      "   Completed epochs: 0\n",
      "\n",
      "ðŸ“Š Performance Assessment:\n",
      "âš ï¸ NEEDS IMPROVEMENT! Consider longer training or ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cell 8: Start Enhanced Training\n",
    "print(\"ðŸŽ¯ Starting Enhanced EfficientNet-B4 Training...\")\n",
    "print(\"   From: Fast Stable B3 (0.7118 AUC)\")\n",
    "print(\"   To: Enhanced B4 (0.85+ AUC target)\")\n",
    "print(\"   Features: Multi-attention, TTA, Mixup/Cutmix, Pyramid pooling\")\n",
    "print(\"   Expected time: 20-25 hours\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Clear CUDA cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Memory monitoring\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸ–¥ï¸ Pre-training GPU memory:\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"   Cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nðŸš€ Training started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Run enhanced training\n",
    "try:\n",
    "    best_auc, best_f1 = trainer.train_for_excellence()\n",
    "    training_completed = True\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¸ï¸ Training interrupted by user\")\n",
    "    best_auc = trainer.best_val_auc\n",
    "    best_f1 = trainer.best_val_f1\n",
    "    training_completed = False\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training error: {e}\")\n",
    "    best_auc = trainer.best_val_auc\n",
    "    best_f1 = trainer.best_val_f1\n",
    "    training_completed = False\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nðŸ† ENHANCED TRAINING SESSION COMPLETED!\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"ðŸŽ¯ Final Results:\")\n",
    "print(f\"   Best AUC: {best_auc:.4f}\")\n",
    "print(f\"   Best Macro-F1: {best_f1:.4f}\")\n",
    "print(f\"   Training time: {training_time/3600:.2f} hours\")\n",
    "print(f\"   Completed epochs: {len(trainer.val_aucs) if hasattr(trainer, 'val_aucs') else 'N/A'}\")\n",
    "if hasattr(trainer, 'val_aucs') and trainer.val_aucs:\n",
    "    print(f\"   Average time per epoch: {training_time/len(trainer.val_aucs)/60:.1f} minutes\")\n",
    "\n",
    "# Performance assessment\n",
    "print(f\"\\nðŸ“Š Performance Assessment:\")\n",
    "if best_auc >= 0.85:\n",
    "    print(f\"âœ… EXCELLENCE ACHIEVED! Target 0.85+ AUC reached!\")\n",
    "    print(f\"ðŸŽ‰ Ready for clinical deployment consideration\")\n",
    "    assessment = \"EXCELLENCE\"\n",
    "elif best_auc >= 0.82:\n",
    "    print(f\"ðŸ”¥ HIGH PERFORMANCE! Very close to excellence\")\n",
    "    print(f\"ðŸ’¡ Consider ensemble or fine-tuning for 0.85+\")\n",
    "    assessment = \"HIGH_PERFORMANCE\"\n",
    "elif best_auc >= 0.80:\n",
    "    print(f\"ðŸ“ˆ GREAT PROGRESS! Significant improvement\")\n",
    "    print(f\"ðŸ’¡ Ready for ensemble combination\")\n",
    "    assessment = \"GREAT_PROGRESS\"\n",
    "elif best_auc >= 0.75:\n",
    "    print(f\"ðŸ“Š GOOD FOUNDATION! Solid improvement achieved\")\n",
    "    print(f\"ðŸ’¡ Consider ensemble approach\")\n",
    "    assessment = \"GOOD_FOUNDATION\"\n",
    "else:\n",
    "    print(f\"âš ï¸ NEEDS IMPROVEMENT! Consider longer training or ensemble\")\n",
    "    assessment = \"NEEDS_IMPROVEMENT\"\n",
    "\n",
    "# Improvement calculation\n",
    "if best_auc > 0.7118:  # B3 baseline\n",
    "    improvement = best_auc - 0.7118\n",
    "    print(f\"\\nðŸ“ˆ Improvement over Fast Stable B3:\")\n",
    "    print(f\"   AUC improvement: +{improvement:.4f}\")\n",
    "    print(f\"   Relative improvement: +{improvement/0.7118*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f8073-6664-478f-9a69-4243b82b2e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
