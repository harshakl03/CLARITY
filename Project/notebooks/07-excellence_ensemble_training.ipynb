{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d37d75-8a66-4452-99f3-2fb82a697f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Excellence modules imported successfully\n",
      "ðŸŽ¯ Target: 0.85+ AUC with excellence metrics\n",
      "ðŸš€ Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 3060\n",
      "   VRAM: 12.9 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Excellence Setup and Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import excellence modules\n",
    "from src.data.dataset import ChestXrayDataset, create_data_splits, calculate_class_weights\n",
    "from src.data.dataloader import create_rtx3060_dataloaders\n",
    "from src.models.excellence_ensemble import (\n",
    "    create_excellence_efficientnet,\n",
    "    ExcellenceLoss\n",
    ")\n",
    "from src.trainer.excellence_trainer import ExcellenceTrainer\n",
    "\n",
    "print(\"âœ… Excellence modules imported successfully\")\n",
    "print(\"ðŸŽ¯ Target: 0.85+ AUC with excellence metrics\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd820dff-df59-40a6-a293-632e0a6f1a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading data pipeline for excellence training...\n",
      "âœ… Metadata loaded: 112,120 entries\n",
      "âœ… Image mapping: 112,120 images\n",
      "Patient-level data splits:\n",
      "  Train: 83,847 images from 23,105 patients (74.8%)\n",
      "  Val:   11,550 images from 3,080 patients (10.3%)\n",
      "  Test:  16,723 images from 4,620 patients (14.9%)\n",
      "âœ… No patient overlap verified - clean splits!\n",
      "âœ… Data splits: Train(83,847) Val(11,550) Test(16,723)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data Pipeline for Excellence Training\n",
    "BASE_PATH = Path(r\"D:/Projects/CLARITY/Model/Dataset/archive\")  # Update this!\n",
    "\n",
    "print(\"ðŸ”„ Loading data pipeline for excellence training...\")\n",
    "\n",
    "# Load metadata\n",
    "data_entry_path = BASE_PATH / \"Data_Entry_2017.csv\"\n",
    "df = pd.read_csv(data_entry_path)\n",
    "print(f\"âœ… Metadata loaded: {len(df):,} entries\")\n",
    "\n",
    "# Create image mapping\n",
    "image_mapping = {}\n",
    "for main_folder in sorted(BASE_PATH.iterdir()):\n",
    "    if main_folder.is_dir() and main_folder.name.startswith('images_'):\n",
    "        images_subfolder = main_folder / 'images'\n",
    "        if images_subfolder.exists():\n",
    "            for img_file in images_subfolder.glob(\"*.png\"):\n",
    "                image_name = img_file.name\n",
    "                if image_name not in image_mapping:\n",
    "                    image_mapping[image_name] = img_file\n",
    "\n",
    "print(f\"âœ… Image mapping: {len(image_mapping):,} images\")\n",
    "\n",
    "# Create data splits\n",
    "train_df, val_df, test_df = create_data_splits(df, \n",
    "                                               test_size=0.15,\n",
    "                                               val_size=0.10,\n",
    "                                               random_seed=42)\n",
    "\n",
    "print(f\"âœ… Data splits: Train({len(train_df):,}) Val({len(val_df):,}) Test({len(test_df):,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c92ab1eb-ab3c-422c-af3d-770b94c2b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating excellence datasets...\n",
      "âš™ï¸  Excellence Configuration:\n",
      "   Image size: 512Ã—512\n",
      "   Batch size: 16\n",
      "   Effective batch: 96\n",
      "   Target: 0.85+ AUC\n",
      "Dataset created with 83847 samples\n",
      "Training mode: True\n",
      "Image size: 512x512\n",
      "\n",
      "Label matrix created: (83847, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............  45146\n",
      "  Atelectasis..............   8720\n",
      "  Cardiomegaly.............   2019\n",
      "  Effusion.................  10071\n",
      "  Infiltration.............  14772\n",
      "  Mass.....................   4477\n",
      "  Nodule...................   4691\n",
      "  Pneumonia................   1062\n",
      "  Pneumothorax.............   3981\n",
      "  Consolidation............   3458\n",
      "  Edema....................   1738\n",
      "  Emphysema................   1794\n",
      "  Fibrosis.................   1236\n",
      "  Pleural_Thickening.......   2562\n",
      "  Hernia...................    171\n",
      "Transforms created for training\n",
      "Dataset created with 11550 samples\n",
      "Training mode: False\n",
      "Image size: 512x512\n",
      "\n",
      "Label matrix created: (11550, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............   6197\n",
      "  Atelectasis..............   1148\n",
      "  Cardiomegaly.............    331\n",
      "  Effusion.................   1311\n",
      "  Infiltration.............   2181\n",
      "  Mass.....................    470\n",
      "  Nodule...................    710\n",
      "  Pneumonia................    149\n",
      "  Pneumothorax.............    511\n",
      "  Consolidation............    517\n",
      "  Edema....................    232\n",
      "  Emphysema................    305\n",
      "  Fibrosis.................    169\n",
      "  Pleural_Thickening.......    326\n",
      "  Hernia...................     13\n",
      "Transforms created for validation\n",
      "Dataset created with 16723 samples\n",
      "Training mode: False\n",
      "Image size: 512x512\n",
      "\n",
      "Label matrix created: (16723, 15)\n",
      "Positive samples per class:\n",
      "  No Finding...............   9018\n",
      "  Atelectasis..............   1691\n",
      "  Cardiomegaly.............    426\n",
      "  Effusion.................   1935\n",
      "  Infiltration.............   2941\n",
      "  Mass.....................    835\n",
      "  Nodule...................    930\n",
      "  Pneumonia................    220\n",
      "  Pneumothorax.............    810\n",
      "  Consolidation............    692\n",
      "  Edema....................    333\n",
      "  Emphysema................    417\n",
      "  Fibrosis.................    281\n",
      "  Pleural_Thickening.......    497\n",
      "  Hernia...................     43\n",
      "Transforms created for validation\n",
      "âœ… Excellence datasets created!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create Excellence Datasets with Optimized Settings\n",
    "print(\"ðŸ”„ Creating excellence datasets...\")\n",
    "\n",
    "# EXCELLENCE SETTINGS - Optimized for maximum performance\n",
    "EXCELLENCE_IMAGE_SIZE = 512  # Higher resolution for better features\n",
    "EXCELLENCE_BATCH_SIZE = 16    # Smaller batch for larger images\n",
    "EXCELLENCE_WORKERS = 4       # Optimized workers\n",
    "EXCELLENCE_ACCUMULATION = 6  # Effective batch = 36\n",
    "\n",
    "print(f\"âš™ï¸  Excellence Configuration:\")\n",
    "print(f\"   Image size: {EXCELLENCE_IMAGE_SIZE}Ã—{EXCELLENCE_IMAGE_SIZE}\")\n",
    "print(f\"   Batch size: {EXCELLENCE_BATCH_SIZE}\")\n",
    "print(f\"   Effective batch: {EXCELLENCE_BATCH_SIZE * EXCELLENCE_ACCUMULATION}\")\n",
    "print(f\"   Target: 0.85+ AUC\")\n",
    "\n",
    "# Create excellence datasets with aggressive augmentation\n",
    "train_dataset = ChestXrayDataset(\n",
    "    df=train_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=EXCELLENCE_IMAGE_SIZE,\n",
    "    is_training=True,\n",
    "    augmentation_prob=0.95  # Very aggressive augmentation\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDataset(\n",
    "    df=val_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=EXCELLENCE_IMAGE_SIZE,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDataset(\n",
    "    df=test_df,\n",
    "    image_mapping=image_mapping,\n",
    "    image_size=EXCELLENCE_IMAGE_SIZE,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Excellence datasets created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77de7424-aa71-445d-a389-e896c2f7a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating excellence dataloaders...\n",
      "\n",
      "Class weights (inverse_freq_sqrt):\n",
      "--------------------------------------------------\n",
      "No Finding...............    1.000 (pos:  45146)\n",
      "Atelectasis..............    1.000 (pos:   8720)\n",
      "Cardiomegaly.............    1.000 (pos:   2019)\n",
      "Effusion.................    1.000 (pos:  10071)\n",
      "Infiltration.............    1.000 (pos:  14772)\n",
      "Mass.....................    1.000 (pos:   4477)\n",
      "Nodule...................    1.000 (pos:   4691)\n",
      "Pneumonia................    1.000 (pos:   1062)\n",
      "Pneumothorax.............    1.000 (pos:   3981)\n",
      "Consolidation............    1.000 (pos:   3458)\n",
      "Edema....................    1.000 (pos:   1738)\n",
      "Emphysema................    1.000 (pos:   1794)\n",
      "Fibrosis.................    1.000 (pos:   1236)\n",
      "Pleural_Thickening.......    1.000 (pos:   2562)\n",
      "Hernia...................    1.000 (pos:    171)\n",
      "âœ… Enhanced class weights calculated\n",
      "Creating RTX 3060 optimized dataloaders:\n",
      "  Batch size: 16\n",
      "  Num workers: 4\n",
      "  Weighted sampling: True\n",
      "  Weighted sampler created with 83847 samples\n",
      "DataLoaders created:\n",
      "  Train: 5240 batches\n",
      "  Val:   722 batches\n",
      "  Test:  1046 batches\n",
      "âœ… Excellence dataloaders created!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Excellence DataLoaders\n",
    "print(\"ðŸ”„ Creating excellence dataloaders...\")\n",
    "\n",
    "# Calculate enhanced class weights\n",
    "class_weights = calculate_class_weights(train_dataset.labels, method='inverse_freq_sqrt')\n",
    "print(f\"âœ… Enhanced class weights calculated\")\n",
    "\n",
    "# Create excellence dataloaders\n",
    "train_loader, val_loader, test_loader = create_rtx3060_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=EXCELLENCE_BATCH_SIZE,\n",
    "    num_workers=EXCELLENCE_WORKERS,\n",
    "    use_weighted_sampling=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Excellence dataloaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18a21974-5433-4c0a-9742-39a0c504c0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating excellence model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Excellence EfficientNet Created:\n",
      "   Backbone: efficientnet_b4\n",
      "   Advanced Attention: True\n",
      "   Multi-scale Fusion: True\n",
      "   Classifier dim: 3072\n",
      "âœ… Excellence Loss Function Created\n",
      "   Focal weight: 0.5\n",
      "   Asymmetric weight: 0.3\n",
      "   BCE weight: 0.2\n",
      "   Components: 3 (consistency disabled)\n",
      "\n",
      "ðŸ“Š Excellence Model Statistics:\n",
      "   Total parameters: 31,730,490\n",
      "   Architecture: Excellence EfficientNet-B4\n",
      "   Features: Multi-scale fusion + Advanced attention\n",
      "   Loss components: 3 (Focal + Asymmetric + BCE)\n",
      "âœ… Excellence model created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Excellence Model (CORRECTED)\n",
    "print(\"ðŸ”„ Creating excellence model...\")\n",
    "\n",
    "# Create high-performance EfficientNet-B4\n",
    "model = create_excellence_efficientnet(\n",
    "    num_classes=15,\n",
    "    model_size='b4',  # High performance backbone\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Create excellence loss function (FIXED - 3 components only)\n",
    "criterion = ExcellenceLoss(\n",
    "    class_weights=class_weights,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=2.5,\n",
    "    label_smoothing=0.15,\n",
    "    loss_weights={\n",
    "        'focal': 0.5,      # Increased weight\n",
    "        'asymmetric': 0.3,\n",
    "        'bce': 0.2         # Only 3 components, sum = 1.0\n",
    "        # Removed 'consistency' to fix the KeyError\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nðŸ“Š Excellence Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Architecture: Excellence EfficientNet-B4\")\n",
    "print(f\"   Features: Multi-scale fusion + Advanced attention\")\n",
    "print(f\"   Loss components: 3 (Focal + Asymmetric + BCE)\")\n",
    "\n",
    "# Enable optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "print(\"âœ… Excellence model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eb4f922-ec9e-432a-a422-fcf8317db260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Initializing excellence trainer...\n",
      "âœ… Mixed precision training enabled (FP16)\n",
      "ðŸš€ Excellence Trainer initialized:\n",
      "   Target: 0.85+ AUC\n",
      "   Max epochs: 20\n",
      "   Batch accumulation: 6\n",
      "   Scheduler: cosine_warm_restarts\n",
      "   Test-time augmentation: True\n",
      "âœ… Excellence trainer initialized!\n",
      "   Target: 0.85+ AUC with 90%+ accuracy\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Initialize Excellence Trainer\n",
    "print(\"ðŸ”„ Initializing excellence trainer...\")\n",
    "\n",
    "# Excellence training configuration\n",
    "trainer = ExcellenceTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    learning_rate=8e-5,\n",
    "    weight_decay=1e-5,\n",
    "    accumulation_steps=EXCELLENCE_ACCUMULATION,\n",
    "    mixed_precision=True,\n",
    "    patience=15,\n",
    "    max_epochs=20,\n",
    "    scheduler_type='cosine_warm_restarts',\n",
    "    warmup_epochs=5,\n",
    "    test_time_augmentation=True,\n",
    "    gradient_clipping=1.0,\n",
    "    checkpoint_dir='../models/excellence_checkpoints'\n",
    ")\n",
    "\n",
    "print(\"âœ… Excellence trainer initialized!\")\n",
    "print(f\"   Target: 0.85+ AUC with 90%+ accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0732b09-8c79-4e11-9128-32c398fd506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Starting CLARITy Excellence Training...\n",
      "   Target: 0.85+ AUC\n",
      "   Expected time: 8-12 hours\n",
      "==========================================================================================\n",
      "ðŸŽ¯ Starting Excellence Training for 0.85+ AUC\n",
      "   Max epochs: 20\n",
      "   Early stopping patience: 15\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.5987\n",
      "Epoch   1/20 | Loss: 0.2001/0.1612 | AUC: 0.5987 | F1: 0.0000 | IoU: 0.0000 | LR: 7.95e-05 | Time: 2547.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.6248\n",
      "Epoch   2/20 | Loss: 0.1781/0.1592 | AUC: 0.6248 | F1: 0.0000 | IoU: 0.0000 | LR: 7.80e-05 | Time: 2313.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/20 | Loss: 0.1749/0.1589 | AUC: 0.6149 | F1: 0.0000 | IoU: 0.0000 | LR: 7.56e-05 | Time: 2399.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.6385\n",
      "Epoch   4/20 | Loss: 0.1730/0.1586 | AUC: 0.6385 | F1: 0.0000 | IoU: 0.0000 | LR: 7.24e-05 | Time: 2407.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.6543\n",
      "Epoch   5/20 | Loss: 0.1713/0.1578 | AUC: 0.6543 | F1: 0.0000 | IoU: 0.0000 | LR: 6.83e-05 | Time: 2216.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.6618\n",
      "Epoch   6/20 | Loss: 0.1699/0.1581 | AUC: 0.6618 | F1: 0.0000 | IoU: 0.0000 | LR: 6.35e-05 | Time: 2213.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.6984\n",
      "Epoch   7/20 | Loss: 0.1687/0.1567 | AUC: 0.6984 | F1: 0.0000 | IoU: 0.0000 | LR: 5.82e-05 | Time: 2219.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.6985\n",
      "Epoch   8/20 | Loss: 0.1678/0.1556 | AUC: 0.6985 | F1: 0.0000 | IoU: 0.0000 | LR: 5.24e-05 | Time: 2614.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.7013\n",
      "Epoch   9/20 | Loss: 0.1670/0.1558 | AUC: 0.7013 | F1: 0.0000 | IoU: 0.0000 | LR: 4.63e-05 | Time: 2217.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.7035\n",
      "Epoch  10/20 | Loss: 0.1662/0.1556 | AUC: 0.7035 | F1: 0.0000 | IoU: 0.0000 | LR: 4.00e-05 | Time: 2213.1s\n",
      "\n",
      "ðŸ“Š Progress Report (Epoch 10):\n",
      "   Best AUC: 0.7035\n",
      "   Current AUC: 0.7035\n",
      "   Mean F1: 0.0000\n",
      "   Mean Precision: 0.0000\n",
      "   Mean Recall: 0.0000\n",
      "   Mean IoU: 0.0000\n",
      "   Overall Accuracy: 0.9159\n",
      "   Top 3 classes:\n",
      "     Pneumothorax: 0.828\n",
      "     Edema: 0.863\n",
      "     Emphysema: 0.869\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž New best model saved: AUC = 0.7146\n",
      "Epoch  11/20 | Loss: 0.1659/0.1550 | AUC: 0.7146 | F1: 0.0000 | IoU: 0.0000 | LR: 3.38e-05 | Time: 2203.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12/20 | Loss: 0.1655/nan | AUC: 0.5000 | F1: 0.0000 | IoU: 0.0000 | LR: 2.77e-05 | Time: 2201.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13/20 | Loss: 0.1650/nan | AUC: 0.5000 | F1: 0.0000 | IoU: 0.0000 | LR: 2.19e-05 | Time: 2201.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14/20 | Loss: 0.1645/nan | AUC: 0.5000 | F1: 0.0000 | IoU: 0.0000 | LR: 1.66e-05 | Time: 2201.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/20 | Loss: 0.1642/nan | AUC: 0.5000 | F1: 0.0000 | IoU: 0.0000 | LR: 1.18e-05 | Time: 2201.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16/20 | Loss: 0.1642/nan | AUC: 0.5000 | F1: 0.0000 | IoU: 0.0000 | LR: 7.71e-06 | Time: 2201.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17/20 | Loss: 0.1641/nan | AUC: 0.5000 | F1: 0.0000 | IoU: 0.0000 | LR: 4.44e-06 | Time: 2201.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18/20 | Loss: 0.1641/nan | AUC: 0.5000 | F1: 0.0000 | IoU: 0.0000 | LR: 2.04e-06 | Time: 2210.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  19/20 | Loss: 0.1637/nan | AUC: 0.5000 | F1: 0.0000 | IoU: 0.0000 | LR: 5.72e-07 | Time: 2211.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Run excellence training\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m best_auc \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_for_excellence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ† EXCELLENCE TRAINING COMPLETED!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\Project\\notebooks\\..\\src\\trainer\\excellence_trainer.py:418\u001b[0m, in \u001b[0;36mExcellenceTrainer.train_for_excellence\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m--> 418\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Validation step\u001b[39;00m\n\u001b[0;32m    421\u001b[0m val_loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\Project\\notebooks\\..\\src\\trainer\\excellence_trainer.py:287\u001b[0m, in \u001b[0;36mExcellenceTrainer.train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monecycle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    285\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 287\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulation_steps\n\u001b[0;32m    288\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 7: Start Excellence Training\n",
    "print(\"ðŸŽ¯ Starting CLARITy Excellence Training...\")\n",
    "print(\"   Target: 0.85+ AUC\")\n",
    "print(\"   Expected time: 8-12 hours\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "# Run excellence training\n",
    "best_auc = trainer.train_for_excellence()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nðŸ† EXCELLENCE TRAINING COMPLETED!\")\n",
    "print(f\"ðŸŽ¯ Final Results:\")\n",
    "print(f\"   Best AUC: {best_auc:.4f}\")\n",
    "print(f\"   Target (0.85): {'âœ… ACHIEVED!' if best_auc >= 0.85 else 'ðŸ”¸ Close' if best_auc >= 0.80 else 'âŒ Needs work'}\")\n",
    "print(f\"   Training time: {training_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf9ed116-b81e-4e43-88a3-2f01ae77e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading best checkpoint...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.28 GiB is allocated by PyTorch, and 110.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[0;32m      8\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/excellence_checkpoints/best_excellence_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load model state\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\CLARITY.env\\lib\\site-packages\\torch\\serialization.py:1525\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1523\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1524\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1525\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1526\u001b[0m             opened_zipfile,\n\u001b[0;32m   1527\u001b[0m             map_location,\n\u001b[0;32m   1528\u001b[0m             pickle_module,\n\u001b[0;32m   1529\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1530\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1531\u001b[0m         )\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1533\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\CLARITY.env\\lib\\site-packages\\torch\\serialization.py:2114\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   2113\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 2114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2115\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2117\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\CLARITY.env\\lib\\site-packages\\torch\\serialization.py:2078\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2077\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 2078\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\CLARITY.env\\lib\\site-packages\\torch\\serialization.py:2044\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mdetect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2044\u001b[0m     wrap_storage \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2046\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_fake_device \u001b[38;5;241m=\u001b[39m location\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\CLARITY.env\\lib\\site-packages\\torch\\serialization.py:1859\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\CLARITY.env\\lib\\site-packages\\torch\\serialization.py:698\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 698\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\CLARITY.env\\lib\\site-packages\\torch\\serialization.py:637\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[0;32m    636\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\CLARITY.env\\lib\\site-packages\\torch\\storage.py:287\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[1;34m(self, device, non_blocking)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[0;32m    286\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Projects\\CLARITY\\Model\\CLARITY.env\\lib\\site-packages\\torch\\_utils.py:101\u001b[0m, in \u001b[0;36m_to\u001b[1;34m(self, device, non_blocking)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse, (\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     )\n\u001b[1;32m--> 101\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.28 GiB is allocated by PyTorch, and 110.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Cell: Resume from Best Checkpoint (FIXED)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"ðŸ”„ Loading best checkpoint...\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"../models/excellence_checkpoints/best_excellence_model.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "# Load model state\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ… Loaded model from epoch {checkpoint['epoch']} with AUC {checkpoint['best_val_auc']:.4f}\")\n",
    "\n",
    "# Create new stable trainer with fixes\n",
    "trainer_resume = ExcellenceTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    \n",
    "    # STABILITY FIXES\n",
    "    learning_rate=2e-5,              # Lower LR\n",
    "    gradient_clipping=0.5,           # Stronger clipping\n",
    "    mixed_precision=False,           # Disable FP16\n",
    "    scheduler_type='cosine_warm_restarts',  # Use cosine instead of plateau\n",
    "    \n",
    "    max_epochs=15,                   # Continue for 15 more epochs\n",
    "    patience=8,\n",
    "    checkpoint_dir=\"../models/excellence_checkpoints_resume\"  # FIXED PATH\n",
    ")\n",
    "\n",
    "# Set starting values from checkpoint\n",
    "trainer_resume.best_val_auc = checkpoint['best_val_auc']\n",
    "trainer_resume.best_epoch = checkpoint['epoch']\n",
    "\n",
    "print(f\"ðŸŽ¯ Ready to resume training!\")\n",
    "print(f\"   Starting AUC: {checkpoint['best_val_auc']:.4f}\")\n",
    "print(f\"   Target: 0.80+ AUC\")\n",
    "\n",
    "# Resume training\n",
    "print(\"ðŸš€ Starting resumed training...\")\n",
    "final_auc = trainer_resume.train_for_excellence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "834d3ecd-c9c7-4b27-a592-a6ddc2769167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving interrupted training state...\n",
      "âœ… Training state saved!\n",
      "   Directory: ..\\models\\interrupted_training\n",
      "   Best AUC achieved: 0.7146\n",
      "   Status: Ready to resume with fixes\n",
      "   Time saved: ~11 hours of training preserved!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Save Current Training State (FIXED)\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ðŸ’¾ Saving interrupted training state...\")\n",
    "\n",
    "# Create save directory\n",
    "save_dir = Path(\"../models/interrupted_training\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save current model state\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'training_interrupted_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'completed_epochs': 11,  # Last successful epoch\n",
    "    'best_auc_achieved': 0.7146,\n",
    "    'training_status': 'interrupted_due_to_nan_loss',\n",
    "    'next_steps': 'resume_with_stability_fixes',\n",
    "    'model_architecture': 'Excellence_EfficientNet_B4',\n",
    "    'training_config': {\n",
    "        'image_size': 512,\n",
    "        'batch_size': 16,\n",
    "        'max_epochs': 20,\n",
    "        'scheduler': 'cosine_warm_restarts'\n",
    "    }\n",
    "}, save_dir / \"interrupted_training_state.pth\")\n",
    "\n",
    "# Save training summary (FIXED - UTF-8 encoding and no Unicode arrows)\n",
    "with open(save_dir / \"training_summary.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"\"\"CLARITy Excellence Training - Interrupted Session Summary\n",
    "    \n",
    "Training Period: 20 epochs planned\n",
    "Completed: 11 successful epochs\n",
    "Status: Interrupted due to NaN loss after epoch 11\n",
    "\n",
    "Performance:\n",
    "- Best AUC: 0.7146 (Epoch 11)\n",
    "- Progress: 59.7% to 71.46% AUC (+11.76%)\n",
    "- Target: 0.85+ AUC\n",
    "- Gap to target: +0.135 AUC needed\n",
    "\n",
    "Top Performing Classes (Epoch 10):\n",
    "- Emphysema: 0.869\n",
    "- Edema: 0.863  \n",
    "- Pneumothorax: 0.828\n",
    "\n",
    "Issue: NaN loss starting from epoch 12\n",
    "Cause: Gradient explosion/numerical instability\n",
    "\n",
    "Next Steps:\n",
    "1. Resume from best checkpoint (epoch 11)\n",
    "2. Apply stability fixes (lower LR, gradient clipping)\n",
    "3. Continue training for 15 more epochs\n",
    "4. Expected final AUC: 0.80-0.85+\n",
    "\n",
    "Files Saved:\n",
    "- Best model: ../models/excellence_checkpoints/best_excellence_model.pth\n",
    "- Interrupted state: ../models/interrupted_training/interrupted_training_state.pth\n",
    "\n",
    "Training Time: ~11 hours for 11 epochs\n",
    "GPU Temperature: 77C (within safe range)\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… Training state saved!\")\n",
    "print(f\"   Directory: {save_dir}\")\n",
    "print(f\"   Best AUC achieved: 0.7146\")\n",
    "print(f\"   Status: Ready to resume with fixes\")\n",
    "print(f\"   Time saved: ~11 hours of training preserved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68e46787-6f1c-4f93-8526-c79532314ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving excellence model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_auc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 14\u001b[0m\n\u001b[0;32m      6\u001b[0m save_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_architecture\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExcellence_EfficientNet_B4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m'\u001b[39m: EXCELLENCE_IMAGE_SIZE,\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_auc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbest_auc\u001b[49m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_config\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8e-5\u001b[39m,\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheduler_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine_warm_restarts\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_time_augmentation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixed_precision\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     },\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_features\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madvanced_attention\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiscale_fusion\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexcellence_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_time_augmentation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     }\n\u001b[0;32m     28\u001b[0m }, save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcellence_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Save training history\u001b[39;00m\n\u001b[0;32m     31\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mval_aucs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: trainer\u001b[38;5;241m.\u001b[39mtrain_losses,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: trainer\u001b[38;5;241m.\u001b[39mlearning_rates[:\u001b[38;5;28mlen\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mval_aucs)]\n\u001b[0;32m     41\u001b[0m })\u001b[38;5;241m.\u001b[39mto_csv(save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcellence_training_history.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_auc' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save Excellence Model\n",
    "print(\"ðŸ’¾ Saving excellence model...\")\n",
    "\n",
    "# Create save directory\n",
    "save_dir = Path(\"../models/saved_models/excellence_ensemble/excellence_model\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_architecture': 'Excellence_EfficientNet_B4',\n",
    "    'num_classes': 15,\n",
    "    'image_size': EXCELLENCE_IMAGE_SIZE,\n",
    "    'best_auc': best_auc,\n",
    "    'training_config': {\n",
    "        'max_epochs': 100,\n",
    "        'learning_rate': 8e-5,\n",
    "        'scheduler_type': 'cosine_warm_restarts',\n",
    "        'test_time_augmentation': True,\n",
    "        'mixed_precision': True\n",
    "    },\n",
    "    'model_features': {\n",
    "        'advanced_attention': True,\n",
    "        'multiscale_fusion': True,\n",
    "        'excellence_loss': True,\n",
    "        'test_time_augmentation': True\n",
    "    }\n",
    "}, save_dir / \"excellence_model.pth\")\n",
    "\n",
    "# Save training history\n",
    "pd.DataFrame({\n",
    "    'epoch': range(1, len(trainer.val_aucs) + 1),\n",
    "    'train_loss': trainer.train_losses,\n",
    "    'val_loss': trainer.val_losses,\n",
    "    'val_auc': trainer.val_aucs,\n",
    "    'val_ap': trainer.val_aps,\n",
    "    'val_f1': trainer.val_f1s,\n",
    "    'val_precision': trainer.val_precisions,\n",
    "    'val_recall': trainer.val_recalls,\n",
    "    'learning_rate': trainer.learning_rates[:len(trainer.val_aucs)]\n",
    "}).to_csv(save_dir / \"excellence_training_history.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Excellence model saved to: {save_dir}\")\n",
    "print(f\"\\nðŸŽ‰ CLARITy Excellence Training Complete!\")\n",
    "print(f\"   Final AUC: {best_auc:.4f}\")\n",
    "print(f\"   Status: {'ðŸ† EXCELLENCE ACHIEVED!' if best_auc >= 0.85 else 'ðŸ¥ˆ HIGH PERFORMANCE' if best_auc >= 0.80 else 'ðŸ¥‰ GOOD PROGRESS'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a34f1-6e2d-448c-92f8-e4b3df59faa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
